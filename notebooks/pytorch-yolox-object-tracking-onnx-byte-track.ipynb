{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562e8e77",
   "metadata": {},
   "source": [
    "## Setting Up Your Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6569af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Install additional dependencies\n",
    "# !pip install pandas pillow opencv-python\n",
    "\n",
    "# # Install ONNX packages\n",
    "# !pip install onnx onnxruntime onnx-simplifier\n",
    "\n",
    "# # Install utility packages\n",
    "# !pip install cjm_psl_utils cjm_pil_utils cjm_byte_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f28533-cfe4-45d8-850e-a7c5063e508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U cjm_byte_track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f835e2b",
   "metadata": {},
   "source": [
    "## Importing the Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da56cda5-9fa1-4a9c-8f29-5f801c9c80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Standard Library dependencies\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "# Import ByteTrack package\n",
    "from cjm_byte_track.core import BYTETracker\n",
    "from cjm_byte_track.matching import match_detections_with_tracks\n",
    "\n",
    "# Import utility functions\n",
    "from cjm_psl_utils.core import download_file\n",
    "from cjm_pil_utils.core import resize_img\n",
    "\n",
    "# Import OpenCV\n",
    "import cv2\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Import PIL for image manipulation\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Import ONNX dependencies\n",
    "import onnx # Import the onnx module\n",
    "import onnxruntime as ort # Import the ONNX Runtime\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2a086-291b-4d60-bf8a-025a60c49d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa8ce96",
   "metadata": {},
   "source": [
    "## Setting Up the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953f868",
   "metadata": {},
   "source": [
    "### Set the Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a2baf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b708c\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b708c_level0_row0\" class=\"row_heading level0 row0\" >Project Directory:</th>\n",
       "      <td id=\"T_b708c_row0_col0\" class=\"data row0 col0\" >pytorch-yolox-object-detector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b708c_level0_row1\" class=\"row_heading level0 row1\" >Checkpoint Directory:</th>\n",
       "      <td id=\"T_b708c_row1_col0\" class=\"data row1 col0\" >pytorch-yolox-object-detector/2023-08-17_16-14-43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f08616ffb20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The name for the project\n",
    "project_name = f\"pytorch-yolox-object-detector\"\n",
    "\n",
    "# The path for the project folder\n",
    "project_dir = Path(f\"./{project_name}/\")\n",
    "\n",
    "# Create the project directory if it does not already exist\n",
    "project_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The path to the checkpoint folder\n",
    "checkpoint_dir = Path(project_dir/f\"2023-08-17_16-14-43\")\n",
    "# checkpoint_dir = Path(project_dir/f\"pretrained-coco\")\n",
    "\n",
    "pd.Series({\n",
    "    \"Project Directory:\": project_dir,\n",
    "    \"Checkpoint Directory:\": checkpoint_dir,\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a96378",
   "metadata": {},
   "source": [
    "### Download a Font File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37213df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./KFOlCnqEu92Fr1MmEU9vAw.ttf already exists and overwrite is set to False.\n"
     ]
    }
   ],
   "source": [
    "# Set the name of the font file\n",
    "font_file = 'KFOlCnqEu92Fr1MmEU9vAw.ttf'\n",
    "\n",
    "# Download the font file\n",
    "download_file(f\"https://fonts.gstatic.com/s/roboto/v30/{font_file}\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9abf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cd8dddc",
   "metadata": {},
   "source": [
    "## Loading the Checkpoint Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb7352",
   "metadata": {},
   "source": [
    "### Load the Colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3560146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The colormap path\n",
    "colormap_path = list(checkpoint_dir.glob('*colormap.json'))[0]\n",
    "\n",
    "# Load the JSON colormap data\n",
    "with open(colormap_path, 'r') as file:\n",
    "        colormap_json = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a dictionary        \n",
    "colormap_dict = {item['label']: item['color'] for item in colormap_json['items']}\n",
    "\n",
    "# Extract the class names from the colormap\n",
    "class_names = list(colormap_dict.keys())\n",
    "\n",
    "# Make a copy of the colormap in integer format\n",
    "int_colors = [tuple(int(c*255) for c in color) for color in colormap_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f83e7cf-dc3b-40f6-932b-5e42c70639ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stride = 32\n",
    "input_dim_slice = slice(2, 4, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f1cfd-6d21-4546-a860-f218e24661ab",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5960f50-446b-488c-810c-f1abf98431d4",
   "metadata": {},
   "source": [
    "### Define Preprocessing Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a67be-393b-4788-8fc0-b6c04c5fdb3c",
   "metadata": {},
   "source": [
    "#### Define a function to convert and resize an OpenCV video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22bed5a4-13ef-47e5-90f5-93edfdcdf50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_resize_image(frame, target_sz):\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    # Resize image without cropping to multiple of the max stride\n",
    "    resized_img = resize_img(rgb_img, target_sz=target_sz, divisor=1)\n",
    "    return rgb_img, resized_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc4465-48b1-4271-9e61-955b1d747c86",
   "metadata": {},
   "source": [
    "#### Define a function to prepare an input image for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99bf68c1-f023-46b7-96f0-e580854e4cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_for_inference(resized_img, max_stride):\n",
    "    # Calculating the input dimensions that multiples of the max stride\n",
    "    input_dims = [dim - dim % max_stride for dim in resized_img.size]\n",
    "    # Calculate the offsets from the resized image dimensions to the input dimensions\n",
    "    offsets = (np.array(resized_img.size) - input_dims) / 2\n",
    "    # Calculate the scale between the source image and the resized image\n",
    "    min_img_scale = min(rgb_img.size) / min(resized_img.size)\n",
    "    # Crop the resized image to the input dimensions\n",
    "    input_img = resized_img.crop(box=[*offsets, *resized_img.size - offsets])\n",
    "    return input_dims, offsets, min_img_scale, input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ccc4ca-a907-4e0c-b03c-254815a58758",
   "metadata": {},
   "source": [
    "### Define Post-Processing Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee2198-3ec1-4879-85ec-9a1cf9da7760",
   "metadata": {},
   "source": [
    "#### Define a function to generate the output grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b75909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_grids_np(height, width, strides=[8,16,32]):\n",
    "    \"\"\"\n",
    "    Generate a numpy array containing grid coordinates and strides for a given height and width.\n",
    "\n",
    "    Args:\n",
    "        height (int): The height of the image.\n",
    "        width (int): The width of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array containing grid coordinates and strides.\n",
    "    \"\"\"\n",
    "\n",
    "    all_coordinates = []\n",
    "\n",
    "    for stride in strides:\n",
    "        # Calculate the grid height and width\n",
    "        grid_height = height // stride\n",
    "        grid_width = width // stride\n",
    "\n",
    "        # Generate grid coordinates\n",
    "        g1, g0 = np.meshgrid(np.arange(grid_height), np.arange(grid_width), indexing='ij')\n",
    "\n",
    "        # Create an array of strides\n",
    "        s = np.full((grid_height, grid_width), stride)\n",
    "\n",
    "        # Stack the coordinates along with the stride\n",
    "        coordinates = np.stack((g0.flatten(), g1.flatten(), s.flatten()), axis=-1)\n",
    "\n",
    "        # Append to the list\n",
    "        all_coordinates.append(coordinates)\n",
    "\n",
    "    # Concatenate all arrays in the list along the first dimension\n",
    "    output_grids = np.concatenate(all_coordinates, axis=0)\n",
    "\n",
    "    return output_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46af61-23f2-41e8-866a-ea8de371cb0a",
   "metadata": {},
   "source": [
    "#### Define a function to calculate bounding boxes and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b471b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_boxes_and_probs(model_output:np.ndarray, output_grids:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the bounding boxes and their probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    model_output (numpy.ndarray): The output of the model.\n",
    "    output_grids (numpy.ndarray): The output grids.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The array containing the bounding box coordinates, class labels, and maximum probabilities.\n",
    "    \"\"\"\n",
    "    # Calculate the bounding box coordinates\n",
    "    box_centroids = (model_output[..., :2] + output_grids[..., :2]) * output_grids[..., 2:]\n",
    "    box_sizes = np.exp(model_output[..., 2:4]) * output_grids[..., 2:]\n",
    "\n",
    "    x0, y0 = [t.squeeze(axis=2) for t in np.split(box_centroids - box_sizes / 2, 2, axis=2)]\n",
    "    w, h = [t.squeeze(axis=2) for t in np.split(box_sizes, 2, axis=2)]\n",
    "\n",
    "    # Calculate the probabilities for each class\n",
    "    box_objectness = model_output[..., 4]\n",
    "    box_cls_scores = model_output[..., 5:]\n",
    "    box_probs = np.expand_dims(box_objectness, -1) * box_cls_scores\n",
    "\n",
    "    # Get the maximum probability and corresponding class for each proposal\n",
    "    max_probs = np.max(box_probs, axis=-1)\n",
    "    labels = np.argmax(box_probs, axis=-1)\n",
    "\n",
    "    return np.array([x0, y0, w, h, labels, max_probs]).transpose((1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0a016-8f3b-447e-86d8-fe343171e6e9",
   "metadata": {},
   "source": [
    "#### Define a function to extract object proposals from the raw model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "564870f9-06b1-4301-a0f5-d408ac209147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_outputs(outputs, input_dims, bbox_conf_thresh):\n",
    "    # Process the model output\n",
    "    outputs = calculate_boxes_and_probs(outputs, generate_output_grids_np(*input_dims))\n",
    "    # Filter the proposals based on the confidence threshold\n",
    "    max_probs = outputs[:, :, -1]\n",
    "    mask = max_probs > bbox_conf_thresh\n",
    "    proposals = outputs[mask]\n",
    "    # Sort the proposals by probability in descending order\n",
    "    proposals = proposals[proposals[..., -1].argsort()][::-1]\n",
    "    return proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1207ea-09ed-42ed-9b3a-0495632666ad",
   "metadata": {},
   "source": [
    "#### Define a function to calculate the intersection-over-union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3cc1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(proposals:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) for all pairs of bounding boxes (x,y,w,h) in 'proposals'.\n",
    "\n",
    "    The IoU is a measure of overlap between two bounding boxes. It is calculated as the area of\n",
    "    intersection divided by the area of union of the two boxes.\n",
    "\n",
    "    Parameters:\n",
    "    proposals (2D np.array): A NumPy array of bounding boxes, where each box is an array [x, y, width, height].\n",
    "\n",
    "    Returns:\n",
    "    iou (2D np.array): The IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate coordinates for the intersection rectangles\n",
    "    x1 = np.maximum(proposals[:, 0], proposals[:, 0][:, None])\n",
    "    y1 = np.maximum(proposals[:, 1], proposals[:, 1][:, None])\n",
    "    x2 = np.minimum(proposals[:, 0] + proposals[:, 2], (proposals[:, 0] + proposals[:, 2])[:, None])\n",
    "    y2 = np.minimum(proposals[:, 1] + proposals[:, 3], (proposals[:, 1] + proposals[:, 3])[:, None])\n",
    "    \n",
    "    # Calculate intersection areas\n",
    "    intersections = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "\n",
    "    # Calculate union areas\n",
    "    areas = proposals[:, 2] * proposals[:, 3]\n",
    "    unions = areas[:, None] + areas - intersections\n",
    "\n",
    "    # Calculate IoUs\n",
    "    iou = intersections / unions\n",
    "\n",
    "    # Return the iou matrix\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00250d67-940c-4855-bbaa-32f3f0b63a7c",
   "metadata": {},
   "source": [
    "#### Define a function to filter bounding box proposals using Non-Maximum Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3ca40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_sorted_boxes(iou:np.ndarray, iou_thresh:float=0.45) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies non-maximum suppression (NMS) to sorted bounding boxes.\n",
    "\n",
    "    It suppresses boxes that have high overlap (as defined by the IoU threshold) with a box that \n",
    "    has a higher score.\n",
    "\n",
    "    Parameters:\n",
    "    iou (np.ndarray): An IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    iou_thresh (float): The IoU threshold for suppression. Boxes with IoU > iou_thresh are suppressed.\n",
    "\n",
    "    Returns:\n",
    "    keep (np.ndarray): The indices of the boxes to keep after applying NMS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask to keep track of boxes\n",
    "    mask = np.ones(iou.shape[0], dtype=bool)\n",
    "\n",
    "    # Apply non-max suppression\n",
    "    for i in range(iou.shape[0]):\n",
    "        if mask[i]:\n",
    "            # Suppress boxes with higher index and IoU > threshold\n",
    "            mask[(iou[i] > iou_thresh) & (np.arange(iou.shape[0]) > i)] = False\n",
    "\n",
    "    # Return the indices of the boxes to keep\n",
    "    return np.arange(iou.shape[0])[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2cb686-21b6-4edf-9b5f-ff2ee78674b6",
   "metadata": {},
   "source": [
    "#### Define a function to annotate an image with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d91cea1-7057-442f-949b-b1b2af44b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes_pil(image, boxes, labels, colors, font, width=2, font_size=18, probs=None):\n",
    "    \"\"\"\n",
    "    Annotates an image with bounding boxes, labels, and optional probability scores.\n",
    "\n",
    "    Parameters:\n",
    "    - image (PIL.Image): The input image on which annotations will be drawn.\n",
    "    - boxes (list of tuples): A list of bounding box coordinates where each tuple is (x, y, w, h).\n",
    "    - labels (list of str): A list of labels corresponding to each bounding box.\n",
    "    - colors (list of str): A list of colors for each bounding box and its corresponding label.\n",
    "    - font (str): Path to the font file to be used for displaying the labels.\n",
    "    - width (int, optional): Width of the bounding box lines. Defaults to 2.\n",
    "    - font_size (int, optional): Size of the font for the labels. Defaults to 18.\n",
    "    - probs (list of float, optional): A list of probability scores corresponding to each label. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    - annotated_image (PIL.Image): The image annotated with bounding boxes, labels, and optional probability scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a reference diagonal\n",
    "    REFERENCE_DIAGONAL = 1000\n",
    "    \n",
    "    # Scale the font size using the hypotenuse of the image\n",
    "    font_size = int(font_size * (np.hypot(*image.size) / REFERENCE_DIAGONAL))\n",
    "    \n",
    "    # Add probability scores to labels if provided\n",
    "    if probs is not None:\n",
    "        labels = [f\"{label}: {prob*100:.2f}%\" for label, prob in zip(labels, probs)]\n",
    "\n",
    "    # Create an ImageDraw object for drawing on the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load the font file (outside the loop)\n",
    "    fnt = ImageFont.truetype(font, font_size)\n",
    "    \n",
    "    # Compute the mean color value for each color\n",
    "    mean_colors = [np.mean(np.array(color)) for color in colors]\n",
    "\n",
    "    # Loop through the bounding boxes, labels, and colors\n",
    "    for box, label, color, mean_color in zip(boxes, labels, colors, mean_colors):\n",
    "        # Get the bounding box coordinates\n",
    "        x, y, w, h = box\n",
    "\n",
    "        # Draw the bounding box on the image\n",
    "        draw.rectangle([x, y, x+w, y+h], outline=color, width=width)\n",
    "        \n",
    "        # Get the size of the label text box\n",
    "        label_w, label_h = draw.textbbox(xy=(0,0), text=label, font=fnt)[2:]\n",
    "        \n",
    "        # Draw the label rectangle on the image\n",
    "        draw.rectangle([x, y-label_h, x+label_w, y], outline=color, fill=color)\n",
    "\n",
    "        # Draw the label text on the image\n",
    "        font_color = 'black' if mean_color > 127.5 else 'white'\n",
    "        draw.multiline_text((x, y-label_h), label, font=fnt, fill=font_color)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a83e62-ab33-48cb-91ed-71c4e814276b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fffabb13",
   "metadata": {},
   "source": [
    "## Performing Inference with ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837f4dd",
   "metadata": {},
   "source": [
    "### Create an Inference Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8719b1c2-17de-4842-b9e1-3fee5ba3f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a filename for the ONNX model\n",
    "onnx_file_path = list(checkpoint_dir.glob('*.onnx'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "025f7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and create an InferenceSession\n",
    "providers = [\n",
    "    'CPUExecutionProvider',\n",
    "    # \"CUDAExecutionProvider\",\n",
    "]\n",
    "sess_options = ort.SessionOptions()\n",
    "session = ort.InferenceSession(onnx_file_path, sess_options=sess_options, providers=providers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09734ca-4a78-4e61-85b2-8394ff105140",
   "metadata": {},
   "source": [
    "### Select a Test Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb8d809-488c-498c-9f3b-87ce6deb86d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./videos/pexels-rodnae-productions-10373924.mp4 already exists and overwrite is set to False.\n"
     ]
    }
   ],
   "source": [
    "video_dir = \"./videos/\"\n",
    "test_video_name = \"pexels-rodnae-productions-10373924.mp4\"\n",
    "# test_video_name = \"cars_on_highway.mp4\"\n",
    "video_path = f\"{video_dir}/{test_video_name}\"\n",
    "\n",
    "test_video_url = f\"https://huggingface.co/datasets/cj-mills/pexels-object-tracking-test-videos/resolve/main/{test_video_name}\"\n",
    "\n",
    "download_file(test_video_url, video_dir, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d619c-2b6d-466d-8b2d-32201aa927d8",
   "metadata": {},
   "source": [
    "### Initialize a `VideoCapture` Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "998946c4-d5cb-4e00-a0cb-ba647fa36037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c988c\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c988c_level0_row0\" class=\"row_heading level0 row0\" >Frame Width:</th>\n",
       "      <td id=\"T_c988c_row0_col0\" class=\"data row0 col0\" >1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c988c_level0_row1\" class=\"row_heading level0 row1\" >Frame Height:</th>\n",
       "      <td id=\"T_c988c_row1_col0\" class=\"data row1 col0\" >1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c988c_level0_row2\" class=\"row_heading level0 row2\" >Frame FPS:</th>\n",
       "      <td id=\"T_c988c_row2_col0\" class=\"data row2 col0\" >29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c988c_level0_row3\" class=\"row_heading level0 row3\" >Frames:</th>\n",
       "      <td id=\"T_c988c_row3_col0\" class=\"data row3 col0\" >226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f07ce98dff0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_width = int(video_capture.get(3))\n",
    "frame_height = int(video_capture.get(4))\n",
    "frame_fps = int(video_capture.get(5))\n",
    "frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "pd.Series({\n",
    "    \"Frame Width:\": frame_width,\n",
    "    \"Frame Height:\": frame_height,\n",
    "    \"Frame FPS:\": frame_fps,\n",
    "    \"Frames:\": frames\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f95783-453d-4d47-9f2f-485feff3e0cc",
   "metadata": {},
   "source": [
    "### Initialize a `VideoWriter` Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edda2696-8083-472b-af49-2acd48842533",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_out_path = f\"{(video_dir)}{Path(video_path).stem}-byte-track.mp4\"\n",
    "video_writer = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'mp4v'), frame_fps, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6913d8-1a72-4af8-a82e-042aea9d3b9a",
   "metadata": {},
   "source": [
    "### Define Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7deefea6-baeb-4414-bfab-16cce20cec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sz = 288\n",
    "# test_sz = 384\n",
    "bbox_conf_thresh = 0.1\n",
    "iou_thresh = 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e23fe0-2760-49af-89c6-341c1636c22a",
   "metadata": {},
   "source": [
    "### Detect, Track, and Annotate Objects in Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15059748-9183-4062-bbbb-c1c8c5724fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8d7636d12e4f14967b00c9e9a66e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tracker = BYTETracker(track_thresh=0.25, track_buffer=30, match_thresh=0.8, frame_rate=frame_fps)\n",
    "\n",
    "with tqdm(total=frames, desc=\"Processing frames\") as pbar:\n",
    "    while video_capture.isOpened():\n",
    "        ret, frame = video_capture.read()\n",
    "        if ret:\n",
    "            \n",
    "            # Preprocess the current frame\n",
    "            rgb_img, resized_img = convert_and_resize_image(frame, test_sz)\n",
    "            \n",
    "            # Get input image data\n",
    "            input_dims, offsets, min_img_scale, input_img = prepare_image_for_inference(resized_img, max_stride)\n",
    "                        \n",
    "            # Convert the existing input image to NumPy format\n",
    "            input_tensor_np = np.array(input_img, dtype=np.float32).transpose((2, 0, 1))[None]/255\n",
    "\n",
    "            # Start performance counter\n",
    "            start_time = time.perf_counter()\n",
    "                        \n",
    "            # Run inference\n",
    "            outputs = session.run(None, {\"input\": input_tensor_np})[0]\n",
    "\n",
    "            # Process the model output\n",
    "            proposals = process_outputs(outputs, input_tensor_np.shape[input_dim_slice], bbox_conf_thresh)\n",
    "            \n",
    "            # Apply non-max suppression to the proposals with the specified threshold\n",
    "            proposal_indices = nms_sorted_boxes(calc_iou(proposals[:, :-2]), iou_thresh)\n",
    "            proposals = proposals[proposal_indices]\n",
    "            \n",
    "            bbox_list = (proposals[:,:4]+[*offsets, 0, 0])*min_img_scale\n",
    "            label_list = [class_names[int(idx)] for idx in proposals[:,4]]\n",
    "            probs_list = proposals[:,5]\n",
    "\n",
    "            # Update tracker with detections.\n",
    "            track_ids = [-1]*len(bbox_list)\n",
    "\n",
    "            # Convert to tlbr format\n",
    "            tlbr_boxes = bbox_list.copy()\n",
    "            tlbr_boxes[:, 2:4] += tlbr_boxes[:, :2]\n",
    "\n",
    "            # Update tracker with detections\n",
    "            tracks = tracker.update(\n",
    "                output_results=np.concatenate([tlbr_boxes, probs_list[:, np.newaxis]], axis=1),\n",
    "                img_info=rgb_img.size,\n",
    "                img_size=rgb_img.size)\n",
    "            track_ids = match_detections_with_tracks(tlbr_boxes=tlbr_boxes, track_ids=track_ids, tracks=tracks)\n",
    "\n",
    "            # End performance counter\n",
    "            end_time = time.perf_counter()\n",
    "            # Calculate the combined FPS for object detection and tracking\n",
    "            fps = 1 / (end_time - start_time)\n",
    "            # Display the frame rate in the progress bar\n",
    "            pbar.set_postfix(fps=fps)\n",
    "\n",
    "            # Filter object detections based on tracking results\n",
    "            bbox_list, label_list, probs_list, track_ids = zip(*[(bbox, label, prob, track_id) \n",
    "                                                                 for bbox, label, prob, track_id \n",
    "                                                                 in zip(bbox_list, label_list, probs_list, track_ids) if track_id != -1])\n",
    "\n",
    "            # Annotate the current frame with bounding boxes and tracking IDs\n",
    "            annotated_img = draw_bboxes_pil(\n",
    "                image=rgb_img, \n",
    "                boxes=bbox_list, \n",
    "                labels=[f\"{track_id}-{label}\" for track_id, label in zip(track_ids, label_list)],\n",
    "                probs=probs_list,\n",
    "                colors=[int_colors[class_names.index(i)] for i in label_list],  \n",
    "                font=font_file,\n",
    "            )\n",
    "            annotated_frame = cv2.cvtColor(np.array(annotated_img), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            video_writer.write(annotated_frame)\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            break\n",
    "video_capture.release()\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d1ffa-d312-445c-8735-30571e938866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
