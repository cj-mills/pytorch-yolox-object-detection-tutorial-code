{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f835e2b",
   "metadata": {},
   "source": [
    "### Importing the Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b5abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Standard Library dependencies\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Tuple, Optional, NamedTuple\n",
    "\n",
    "# ByteTrack package for object tracking\n",
    "from cjm_byte_track.core import BYTETracker\n",
    "from cjm_byte_track.matching import match_detections_with_tracks\n",
    "\n",
    "# Utility functions\n",
    "from cjm_psl_utils.core import download_file  # For downloading files\n",
    "from cjm_pil_utils.core import resize_img  # For resizing images\n",
    "\n",
    "# OpenCV for computer vision tasks\n",
    "import cv2\n",
    "\n",
    "# NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# PIL (Python Imaging Library) for image processing\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Import Hailo Runtime dependencies\n",
    "from hailo_platform import (\n",
    "    HEF,\n",
    "    ConfigureParams,\n",
    "    FormatType,\n",
    "    HailoSchedulingAlgorithm,\n",
    "    HailoStreamInterface,\n",
    "    InferVStreams,\n",
    "    InputVStreamParams,\n",
    "    OutputVStreamParams,\n",
    "    VDevice\n",
    ")\n",
    "\n",
    "# Import Picamera2\n",
    "from picamera2 import Picamera2, Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4da3df-c466-4bc5-abb4-01e0b73bf882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbc5f440-c748-4ba0-a86e-515761c04ca2",
   "metadata": {},
   "source": [
    "#### Define Functions to Handle Arbitrary Input Resolutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7600f7-ae74-4d7b-96c5-60428bc0fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformData(NamedTuple):\n",
    "    \"\"\"\n",
    "    A data class that stores transformation information applied to an image.\n",
    "\n",
    "    Attributes:\n",
    "        offset (Tuple[int, int]): The (x, y) offset where the resized image was pasted.\n",
    "        scale (float): The scaling factor applied to the original image.\n",
    "    \"\"\"\n",
    "    offset: Tuple[int, int]\n",
    "    scale: float\n",
    "\n",
    "def resize_and_pad(\n",
    "    image: np.ndarray,\n",
    "    target_sz: Tuple[int, int],\n",
    "    return_transform_data: bool = False,\n",
    "    fill_color: Tuple[int, int, int] = (255, 255, 255)\n",
    ") -> Tuple[np.ndarray, Optional[ImageTransformData]]:\n",
    "    \"\"\"\n",
    "    Resize an image while maintaining its aspect ratio and pad it to fit the target size.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The original image as a numpy array.\n",
    "        target_sz (Tuple[int, int]): The desired size (width, height) for the output image.\n",
    "        return_transform_data (bool, optional): If True, returns transformation data (offset and scale).\n",
    "        fill_color (Tuple[int, int, int], optional): The color to use for padding (default is white).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, Optional[ImageTransformData]]: The resized and padded image,\n",
    "        and optionally the transformation data.\n",
    "    \"\"\"\n",
    "    target_width, target_height = target_sz\n",
    "    orig_height, orig_width = image.shape[:2]\n",
    "    \n",
    "    aspect_ratio = orig_width / orig_height\n",
    "    target_aspect_ratio = target_width / target_height\n",
    "\n",
    "    if aspect_ratio > target_aspect_ratio:\n",
    "        new_width = target_width\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        scale = target_width / orig_width\n",
    "    else:\n",
    "        new_height = target_height\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        scale = target_height / orig_height\n",
    "\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    paste_x = (target_width - new_width) // 2\n",
    "    paste_y = (target_height - new_height) // 2\n",
    "\n",
    "    padded_image = np.full((target_height, target_width, 3), fill_color, dtype=np.uint8)\n",
    "    padded_image[paste_y:paste_y+new_height, paste_x:paste_x+new_width] = resized_image\n",
    "\n",
    "    if return_transform_data:\n",
    "        transform_data = ImageTransformData(offset=(paste_x, paste_y), scale=scale)\n",
    "        return padded_image, transform_data\n",
    "    else:\n",
    "        return padded_image, None\n",
    "\n",
    "def adjust_bbox(\n",
    "    bbox: Tuple[float, float, float, float],\n",
    "    transform_data: ImageTransformData\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Adjust a bounding box according to the transformation data (offset and scale).\n",
    "\n",
    "    Args:\n",
    "        bbox (Tuple[float, float, float, float]): The original bounding box as (x, y, width, height).\n",
    "        transform_data (ImageTransformData): The transformation data containing offset and scale.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float, float]: The adjusted bounding box.\n",
    "    \"\"\"\n",
    "    # Unpack the bounding box coordinates and size\n",
    "    x, y, w, h = bbox\n",
    "\n",
    "    # Unpack the transformation data\n",
    "    offset_x, offset_y = transform_data.offset\n",
    "    scale = transform_data.scale\n",
    "\n",
    "    # Adjust the coordinates by subtracting the offset and dividing by the scale\n",
    "    adjusted_x = (x - offset_x) / scale\n",
    "    adjusted_y = (y - offset_y) / scale\n",
    "\n",
    "    # Adjust the size by dividing by the scale\n",
    "    adjusted_w = w / scale\n",
    "    adjusted_h = h / scale\n",
    "\n",
    "    return (adjusted_x, adjusted_y, adjusted_w, adjusted_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8133b06-b5ac-49e5-87db-0edd6f04fb86",
   "metadata": {},
   "source": [
    "#### Define Postprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d386c140-5ba9-4174-b982-276eea666beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_grids_np(height, width, strides=[8,16,32]):\n",
    "    \"\"\"\n",
    "    Generate a numpy array containing grid coordinates and strides for a given height and width.\n",
    "\n",
    "    Args:\n",
    "        height (int): The height of the image.\n",
    "        width (int): The width of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array containing grid coordinates and strides.\n",
    "    \"\"\"\n",
    "\n",
    "    all_coordinates = []\n",
    "\n",
    "    for stride in strides:\n",
    "        # Calculate the grid height and width\n",
    "        grid_height = height // stride\n",
    "        grid_width = width // stride\n",
    "\n",
    "        # Generate grid coordinates\n",
    "        g1, g0 = np.meshgrid(np.arange(grid_height), np.arange(grid_width), indexing='ij')\n",
    "\n",
    "        # Create an array of strides\n",
    "        s = np.full((grid_height, grid_width), stride)\n",
    "\n",
    "        # Stack the coordinates along with the stride\n",
    "        coordinates = np.stack((g0.flatten(), g1.flatten(), s.flatten()), axis=-1)\n",
    "\n",
    "        # Append to the list\n",
    "        all_coordinates.append(coordinates)\n",
    "\n",
    "    # Concatenate all arrays in the list along the first dimension\n",
    "    output_grids = np.concatenate(all_coordinates, axis=0)\n",
    "\n",
    "    return output_grids\n",
    "\n",
    "def calculate_boxes_and_probs(model_output:np.ndarray, output_grids:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the bounding boxes and their probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    model_output (numpy.ndarray): The output of the model.\n",
    "    output_grids (numpy.ndarray): The output grids.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The array containing the bounding box coordinates, class labels, and maximum probabilities.\n",
    "    \"\"\"\n",
    "    # Calculate the bounding box coordinates\n",
    "    box_centroids = (model_output[..., :2] + output_grids[..., :2]) * output_grids[..., 2:]\n",
    "    box_sizes = np.exp(model_output[..., 2:4]) * output_grids[..., 2:]\n",
    "\n",
    "    x0, y0 = [t.squeeze(axis=2) for t in np.split(box_centroids - box_sizes / 2, 2, axis=2)]\n",
    "    w, h = [t.squeeze(axis=2) for t in np.split(box_sizes, 2, axis=2)]\n",
    "\n",
    "    # Calculate the probabilities for each class\n",
    "    box_objectness = model_output[..., 4]\n",
    "    box_cls_scores = model_output[..., 5:]\n",
    "    box_probs = np.expand_dims(box_objectness, -1) * box_cls_scores\n",
    "\n",
    "    # Get the maximum probability and corresponding class for each proposal\n",
    "    max_probs = np.max(box_probs, axis=-1)\n",
    "    labels = np.argmax(box_probs, axis=-1)\n",
    "\n",
    "    return np.array([x0, y0, w, h, labels, max_probs]).transpose((1, 2, 0))\n",
    "\n",
    "def process_outputs(outputs:np.ndarray, input_dims:tuple, bbox_conf_thresh:float):\n",
    "\n",
    "    \"\"\"\n",
    "    Process the model outputs to generate bounding box proposals filtered by confidence threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - outputs (numpy.ndarray): The raw output from the model, which will be processed to calculate boxes and probabilities.\n",
    "    - input_dims (tuple of int): Dimensions (height, width) of the input image to the model.\n",
    "    - bbox_conf_thresh (float): Threshold for the bounding box confidence/probability. Bounding boxes with a confidence\n",
    "                                score below this threshold will be discarded.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.array: An array of proposals where each proposal is an array containing bounding box coordinates\n",
    "                   and its associated probability, sorted in descending order by probability.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the model output\n",
    "    outputs = calculate_boxes_and_probs(outputs, generate_output_grids_np(*input_dims))\n",
    "    # Filter the proposals based on the confidence threshold\n",
    "    max_probs = outputs[:, :, -1]\n",
    "    mask = max_probs > bbox_conf_thresh\n",
    "    proposals = outputs[mask]\n",
    "    # Sort the proposals by probability in descending order\n",
    "    proposals = proposals[proposals[..., -1].argsort()][::-1]\n",
    "    return proposals\n",
    "\n",
    "def calc_iou(proposals:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) for all pairs of bounding boxes (x,y,w,h) in 'proposals'.\n",
    "\n",
    "    The IoU is a measure of overlap between two bounding boxes. It is calculated as the area of\n",
    "    intersection divided by the area of union of the two boxes.\n",
    "\n",
    "    Parameters:\n",
    "    proposals (2D np.array): A NumPy array of bounding boxes, where each box is an array [x, y, width, height].\n",
    "\n",
    "    Returns:\n",
    "    iou (2D np.array): The IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate coordinates for the intersection rectangles\n",
    "    x1 = np.maximum(proposals[:, 0], proposals[:, 0][:, None])\n",
    "    y1 = np.maximum(proposals[:, 1], proposals[:, 1][:, None])\n",
    "    x2 = np.minimum(proposals[:, 0] + proposals[:, 2], (proposals[:, 0] + proposals[:, 2])[:, None])\n",
    "    y2 = np.minimum(proposals[:, 1] + proposals[:, 3], (proposals[:, 1] + proposals[:, 3])[:, None])\n",
    "    \n",
    "    # Calculate intersection areas\n",
    "    intersections = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "\n",
    "    # Calculate union areas\n",
    "    areas = proposals[:, 2] * proposals[:, 3]\n",
    "    unions = areas[:, None] + areas - intersections\n",
    "\n",
    "    # Calculate IoUs\n",
    "    iou = intersections / unions\n",
    "\n",
    "    # Return the iou matrix\n",
    "    return iou\n",
    "\n",
    "def nms_sorted_boxes(iou:np.ndarray, iou_thresh:float=0.45) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies non-maximum suppression (NMS) to sorted bounding boxes.\n",
    "\n",
    "    It suppresses boxes that have high overlap (as defined by the IoU threshold) with a box that \n",
    "    has a higher score.\n",
    "\n",
    "    Parameters:\n",
    "    iou (np.ndarray): An IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    iou_thresh (float): The IoU threshold for suppression. Boxes with IoU > iou_thresh are suppressed.\n",
    "\n",
    "    Returns:\n",
    "    keep (np.ndarray): The indices of the boxes to keep after applying NMS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask to keep track of boxes\n",
    "    mask = np.ones(iou.shape[0], dtype=bool)\n",
    "\n",
    "    # Apply non-max suppression\n",
    "    for i in range(iou.shape[0]):\n",
    "        if mask[i]:\n",
    "            # Suppress boxes with higher index and IoU > threshold\n",
    "            mask[(iou[i] > iou_thresh) & (np.arange(iou.shape[0]) > i)] = False\n",
    "\n",
    "    # Return the indices of the boxes to keep\n",
    "    return np.arange(iou.shape[0])[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673bd685-22f8-4e48-9a17-8c7ef05ec33d",
   "metadata": {},
   "source": [
    "#### Define Bounding Box Annotation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f6df24-e9d8-4512-bafa-ae20d62cf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes_pil(image, boxes, labels, colors, font, width:int=2, font_size:int=18, probs=None):\n",
    "    \"\"\"\n",
    "    Annotates an image with bounding boxes, labels, and optional probability scores.\n",
    "\n",
    "    This function draws bounding boxes on the provided image using the given box coordinates, \n",
    "    colors, and labels. If probabilities are provided, they will be added to the labels.\n",
    "\n",
    "    Parameters:\n",
    "    image (PIL.Image): The input image on which annotations will be drawn.\n",
    "    boxes (list of tuples): A list of bounding box coordinates where each tuple is (x, y, w, h).\n",
    "    labels (list of str): A list of labels corresponding to each bounding box.\n",
    "    colors (list of str): A list of colors for each bounding box and its corresponding label.\n",
    "    font (str): Path to the font file to be used for displaying the labels.\n",
    "    width (int, optional): Width of the bounding box lines. Defaults to 2.\n",
    "    font_size (int, optional): Size of the font for the labels. Defaults to 25.\n",
    "    probs (list of float, optional): A list of probability scores corresponding to each label. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    annotated_image (PIL.Image): The image annotated with bounding boxes, labels, and optional probability scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a reference diagonal\n",
    "    REFERENCE_DIAGONAL = 1000\n",
    "    \n",
    "    # Scale the font size using the hypotenuse of the image\n",
    "    font_size = int(font_size * (np.hypot(*image.size) / REFERENCE_DIAGONAL))\n",
    "    \n",
    "    # Add probability scores to labels\n",
    "    if probs is not None:\n",
    "        labels = [f\"{label}: {prob*100:.2f}%\" for label, prob in zip(labels, probs)]\n",
    "    \n",
    "    # Create a copy of the image\n",
    "    annotated_image = image.copy()\n",
    "\n",
    "    # Create an ImageDraw object for drawing on the image\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "    # Loop through the bounding boxes and labels in the 'annotation' DataFrame\n",
    "    for i in range(len(labels)):\n",
    "        # Get the bounding box coordinates\n",
    "        x, y, w, h = boxes[i]\n",
    "\n",
    "        # Create a tuple of coordinates for the bounding box\n",
    "        shape = (x, y, x+w, y+h)\n",
    "\n",
    "        # Draw the bounding box on the image\n",
    "        draw.rectangle(shape, outline=colors[i], width=width)\n",
    "        \n",
    "        # Load the font file\n",
    "        fnt = ImageFont.truetype(font, font_size)\n",
    "        \n",
    "        # Draw the label box on the image\n",
    "        label_w, label_h = draw.textbbox(xy=(0,0), text=labels[i], font=fnt)[2:]\n",
    "        draw.rectangle((x, y-label_h, x+label_w, y), outline=colors[i], fill=colors[i], width=width)\n",
    "\n",
    "        # Draw the label on the image\n",
    "        draw.multiline_text((x, y-label_h), labels[i], font=fnt, fill='black' if np.mean(colors[i]) > 127.5 else 'white')\n",
    "        \n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cebcf9-509f-4175-b564-839dff9c8680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa8ce96",
   "metadata": {},
   "source": [
    "## Setting Up the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953f868",
   "metadata": {},
   "source": [
    "### Set the Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a2baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the checkpoint folder\n",
    "checkpoint_dir = Path(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a96378",
   "metadata": {},
   "source": [
    "### Download a Font File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37213df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./KFOlCnqEu92Fr1MmEU9vAw.ttf already exists and overwrite is set to False.\n"
     ]
    }
   ],
   "source": [
    "# Set the name of the font file\n",
    "font_file = 'KFOlCnqEu92Fr1MmEU9vAw.ttf'\n",
    "\n",
    "# Download the font file\n",
    "download_file(f\"https://fonts.gstatic.com/s/roboto/v30/{font_file}\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9abf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cd8dddc",
   "metadata": {},
   "source": [
    "## Loading the Inference Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb7352",
   "metadata": {},
   "source": [
    "### Load the Colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3560146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The colormap path\n",
    "colormap_path = list(checkpoint_dir.glob('*colormap.json'))[0]\n",
    "\n",
    "# Load the JSON colormap data\n",
    "with open(colormap_path, 'r') as file:\n",
    "        colormap_json = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a dictionary        \n",
    "colormap_dict = {item['label']: item['color'] for item in colormap_json['items']}\n",
    "\n",
    "# Extract the class names from the colormap\n",
    "class_names = list(colormap_dict.keys())\n",
    "\n",
    "# Make a copy of the colormap in integer format\n",
    "int_colors = [tuple(int(c*255) for c in color) for color in colormap_dict.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d183a-1aeb-4a04-91c3-2d59bc1531ac",
   "metadata": {},
   "source": [
    "### Load the Compiled HEF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7127bc-4391-4436-bf4d-a171c889961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The HEF (Hailo Executable Format) model path\n",
    "hef_file_path = list(checkpoint_dir.glob('*.hef'))[0]\n",
    "\n",
    "# Load the compiled HEF to Hailo device\n",
    "hef = HEF(str(hef_file_path))\n",
    "\n",
    "# Set VDevice (Virtual Device) params to disable the HailoRT service feature\n",
    "params = VDevice.create_params()\n",
    "params.scheduling_algorithm = HailoSchedulingAlgorithm.NONE\n",
    "\n",
    "# Create a Hailo virtual device with the specified parameters\n",
    "target = VDevice(params=params)\n",
    "\n",
    "# Get the \"network groups\" (connectivity groups, aka. \"different networks\") information from the .hef\n",
    "# Configure the device with the HEF and PCIe interface\n",
    "configure_params = ConfigureParams.create_from_hef(hef=hef, interface=HailoStreamInterface.PCIe)\n",
    "network_groups = target.configure(hef, configure_params)\n",
    "\n",
    "# Select the first network group (there's only one in this case)\n",
    "network_group = network_groups[0]\n",
    "network_group_params = network_group.create_params()\n",
    "\n",
    "# Create input and output virtual streams params\n",
    "# These specify the format of the input and output data (in this case, 32-bit float)\n",
    "input_vstreams_params = InputVStreamParams.make(network_group, format_type=FormatType.FLOAT32)\n",
    "output_vstreams_params = OutputVStreamParams.make(network_group, format_type=FormatType.FLOAT32)\n",
    "\n",
    "# Get information about the input and output virtual streams\n",
    "input_vstream_info = hef.get_input_vstream_infos()[0]\n",
    "output_vstream_info = hef.get_output_vstream_infos()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c478953-2a4c-4156-bc0b-33ae6780bc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b3b7b73-fbf2-4a23-9c2c-5d3982c1e6e8",
   "metadata": {},
   "source": [
    "### Define Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c81a4da4-4f0c-4ad9-8c5f-3069fcf55998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired preview size\n",
    "preview_width, preview_height = 960, 540\n",
    "\n",
    "bbox_conf_thresh = 0.35\n",
    "iou_thresh = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b391e3-75ee-45a8-879a-880435c6612d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd57892-80d6-4766-8e83-6ae1b968e088",
   "metadata": {},
   "source": [
    "### Detect, Track, and Annotate Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2772b04-c422-4175-ba44-ec142a1cfa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1:30:25.480652426] [128869] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera_manager.cpp:325 \u001b[0mlibcamera v0.3.2+27-7330f29b\n",
      "[1:30:25.488873181] [128898] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:695 \u001b[0mlibpisp version v1.0.7 28196ed6edcf 29-08-2024 (16:33:32)\n",
      "[1:30:25.500040780] [128898] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1154 \u001b[0mRegistered camera /base/axi/pcie@120000/rp1/i2c@80000/imx708@1a to CFE device /dev/media0 and ISP device /dev/media1 using PiSP variant BCM2712_C0\n",
      "[1:30:25.504580397] [128869] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1197 \u001b[0mconfiguring streams: (0) 640x480-XBGR8888 (1) 1536x864-BGGR_PISP_COMP1\n",
      "[1:30:25.504707471] [128898] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1450 \u001b[0mSensor: /base/axi/pcie@120000/rp1/i2c@80000/imx708@1a - Selected sensor format: 1536x864-SBGGR10_1X10 - Selected CFE format: 1536x864-PC1B\n",
      "[1:30:25.509482661] [128869] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1197 \u001b[0mconfiguring streams: (0) 640x480-XBGR8888 (1) 2304x1296-BGGR_PISP_COMP1\n",
      "[1:30:25.509651716] [128898] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1450 \u001b[0mSensor: /base/axi/pcie@120000/rp1/i2c@80000/imx708@1a - Selected sensor format: 2304x1296-SBGGR10_1X10 - Selected CFE format: 2304x1296-PC1B\n",
      "[1:30:25.515647718] [128869] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1197 \u001b[0mconfiguring streams: (0) 640x480-XBGR8888 (1) 4608x2592-BGGR_PISP_COMP1\n",
      "[1:30:25.515820199] [128898] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1450 \u001b[0mSensor: /base/axi/pcie@120000/rp1/i2c@80000/imx708@1a - Selected sensor format: 4608x2592-SBGGR10_1X10 - Selected CFE format: 4608x2592-PC1B\n",
      "[1:30:25.527309279] [128869] \u001b[1;32m INFO \u001b[1;37mCamera \u001b[1;34mcamera.cpp:1197 \u001b[0mconfiguring streams: (0) 1536x864-RGB888 (1) 1536x864-BGGR_PISP_COMP1\n",
      "[1:30:25.527468649] [128898] \u001b[1;32m INFO \u001b[1;37mRPI \u001b[1;34mpisp.cpp:1450 \u001b[0mSensor: /base/axi/pcie@120000/rp1/i2c@80000/imx708@1a - Selected sensor format: 1536x864-SBGGR10_1X10 - Selected CFE format: 1536x864-PC1B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sensor modes:\n",
      "Mode 0: (1536, 864)x120.13fps\n",
      "Mode 1: (2304, 1296)x56.03fps\n",
      "Mode 2: (4608, 2592)x14.35fps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "# Set up window title for display\n",
    "window_title = \"Camera Feed - Press 'q' to Quit\"\n",
    "\n",
    "# Create a Picamera2 object\n",
    "picam2 = Picamera2()\n",
    "\n",
    "# Print available sensor modes\n",
    "print(\"Available sensor modes:\")\n",
    "for i, mode in enumerate(picam2.sensor_modes):\n",
    "    print(f\"Mode {i}: {mode['size']}x{mode['fps']}fps\")\n",
    "\n",
    "# Choose a mode (let's say we want the second mode, index 1)\n",
    "chosen_mode = picam2.sensor_modes[0]\n",
    "\n",
    "# Create a configuration using the chosen mode\n",
    "config = picam2.create_preview_configuration(\n",
    "    main={\n",
    "        \"size\": chosen_mode[\"size\"],\n",
    "        \"format\": \"RGB888\"\n",
    "    },\n",
    "    controls={\n",
    "        \"FrameRate\": chosen_mode[\"fps\"]\n",
    "    },\n",
    "    sensor={\n",
    "        \"output_size\": chosen_mode[\"size\"],\n",
    "        \"bit_depth\": chosen_mode[\"bit_depth\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure the camera\n",
    "picam2.configure(config)\n",
    "\n",
    "# Start the camera\n",
    "picam2.start()\n",
    "\n",
    "# Initialize the ByteTracker for object tracking\n",
    "tracker = BYTETracker(track_thresh=0.25, track_buffer=30, match_thresh=0.8, frame_rate=30)\n",
    "\n",
    "try:\n",
    "    \n",
    "    # Main processing loop\n",
    "    while True:\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Capture a frame\n",
    "        frame = picam2.capture_array()\n",
    "\n",
    "        # Resize and pad the sample image to the desired input size, retrieving transformation data.\n",
    "        input_img_np, transform_data = resize_and_pad(frame, input_vstream_info.shape[::-1][1:], True)\n",
    "        \n",
    "        # Convert the input image to NumPy format for the model\n",
    "        input_tensor_np = np.array(input_img_np, dtype=np.float32)[None]/255\n",
    "        input_tensor_np = np.ascontiguousarray(input_tensor_np)\n",
    "                        \n",
    "        # Run inference\n",
    "        input_data = {input_vstream_info.name: input_tensor_np}\n",
    "        with InferVStreams(network_group, input_vstreams_params, output_vstreams_params) as infer_pipeline:\n",
    "            with network_group.activate(network_group_params):\n",
    "                infer_results = infer_pipeline.infer(input_data)\n",
    "\n",
    "        # Transpose and extract the first element of the quantized results\n",
    "        outputs = infer_results[output_vstream_info.name].transpose(0, 1, 3, 2)[0]\n",
    "    \n",
    "        # Process the model output to get object proposals\n",
    "        proposals = process_outputs(outputs, input_vstream_info.shape[:-1], bbox_conf_thresh)\n",
    "        \n",
    "        # Apply non-max suppression to filter overlapping proposals\n",
    "        proposal_indices = nms_sorted_boxes(calc_iou(proposals[:, :-2]), iou_thresh)\n",
    "        proposals = proposals[proposal_indices]\n",
    "        \n",
    "        # Extract bounding boxes, labels, and probabilities from proposals\n",
    "        bbox_list = [adjust_bbox(bbox, transform_data) for bbox in proposals[:,:4]]\n",
    "        label_list = [class_names[int(idx)] for idx in proposals[:,4]]\n",
    "        probs_list = proposals[:,5]\n",
    "\n",
    "        # Initialize track IDs for detected objects\n",
    "        track_ids = [-1]*len(bbox_list)\n",
    "\n",
    "        # Convert bounding boxes to top-left bottom-right (tlbr) format\n",
    "        tlbr_boxes = np.array(bbox_list).reshape(-1,4).copy()\n",
    "        tlbr_boxes[:, 2:4] += tlbr_boxes[:, :2]\n",
    "\n",
    "        # Update tracker with detections\n",
    "        tracks = tracker.update(\n",
    "            output_results=np.concatenate([tlbr_boxes, probs_list[:, np.newaxis]], axis=1),\n",
    "            img_info=frame.shape[::-1][1:],\n",
    "            img_size=frame.shape[::-1][1:])\n",
    "\n",
    "        if len(tlbr_boxes) > 0 and len(tracks) > 0:\n",
    "            # Match detections with tracks\n",
    "            track_ids = match_detections_with_tracks(tlbr_boxes=tlbr_boxes, track_ids=track_ids, tracks=tracks)\n",
    "    \n",
    "            # Filter object detections based on tracking results\n",
    "            bbox_list, label_list, probs_list, track_ids = zip(*[(bbox, label, prob, track_id) \n",
    "                                                                for bbox, label, prob, track_id \n",
    "                                                                in zip(bbox_list, label_list, probs_list, track_ids) if track_id != -1])\n",
    "            \n",
    "            if len(bbox_list) > 0:\n",
    "                # Annotate the current frame with bounding boxes and tracking IDs\n",
    "                annotated_img = draw_bboxes_pil(\n",
    "                    image=Image.fromarray(frame), \n",
    "                    boxes=bbox_list, \n",
    "                    labels=[f\"{track_id}-{label}\" for track_id, label in zip(track_ids, label_list)],\n",
    "                    probs=probs_list,\n",
    "                    colors=[int_colors[class_names.index(i)] for i in label_list],  \n",
    "                    font=font_file,\n",
    "                )\n",
    "                annotated_frame = cv2.resize(np.array(annotated_img), (preview_width, preview_height))\n",
    "        else:\n",
    "            # If no detections, use the original frame\n",
    "            annotated_frame = cv2.resize(frame, (preview_width, preview_height))\n",
    "        \n",
    "        # Calculate and display FPS\n",
    "        end_time = time.perf_counter()\n",
    "        processing_time = end_time - start_time\n",
    "        fps = 1 / processing_time\n",
    "    \n",
    "        fps_text = f\"FPS: {fps:.2f}\"\n",
    "        cv2.putText(annotated_frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(window_title, annotated_frame)\n",
    "        \n",
    "        # Check for 'q' key press to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Stop the camera and close the preview window\n",
    "    cv2.destroyAllWindows()\n",
    "    picam2.close()\n",
    "    target.release()  # Release the Hailo device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd6b55-b98b-442a-aaf4-0f0f77e8d7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
