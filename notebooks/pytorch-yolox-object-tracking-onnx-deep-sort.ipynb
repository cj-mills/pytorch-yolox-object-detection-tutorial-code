{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562e8e77",
   "metadata": {},
   "source": [
    "## Setting Up Your Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6569af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Install PyTorch with CUDA\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# # Install additional dependencies\n",
    "# !pip install pandas pillow opencv-python deep-sort-realtime\n",
    "\n",
    "# # Install ONNX packages\n",
    "# !pip install onnx onnxruntime\n",
    "\n",
    "# # Install utility packages\n",
    "# !pip install cjm_psl_utils cjm_pil_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f835e2b",
   "metadata": {},
   "source": [
    "## Importing the Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db09bf00-d5be-4975-8642-f40fe1a1786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Standard Library dependencies\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Import utility functions\n",
    "from cjm_psl_utils.core import download_file\n",
    "from cjm_pil_utils.core import resize_img\n",
    "\n",
    "# Import YOLOX package\n",
    "from cjm_yolox_pytorch.model import build_model\n",
    "from cjm_yolox_pytorch.inference import YOLOXInferenceWrapper\n",
    "\n",
    "# Import OpenCV\n",
    "import cv2\n",
    "\n",
    "# Class for displaying videos in Jupyter notebooks\n",
    "from IPython.display import Video\n",
    "\n",
    "# Import DeepSORT package\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort, EMBEDDER_CHOICES\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Import PIL for image manipulation\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Import PyTorch dependencies\n",
    "import torch\n",
    "\n",
    "# Import ONNX dependencies\n",
    "import onnx # Import the onnx module\n",
    "from onnxsim import simplify # Import the method to simplify ONNX models\n",
    "import onnxruntime as ort # Import the ONNX Runtime\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d3777-e37d-4843-807e-d602907eeee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa8ce96",
   "metadata": {},
   "source": [
    "## Setting Up the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953f868",
   "metadata": {},
   "source": [
    "### Set the Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a2baf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f04aa\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f04aa_level0_row0\" class=\"row_heading level0 row0\" >Project Directory:</th>\n",
       "      <td id=\"T_f04aa_row0_col0\" class=\"data row0 col0\" >pytorch-yolox-object-detector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f04aa_level0_row1\" class=\"row_heading level0 row1\" >Checkpoint Directory:</th>\n",
       "      <td id=\"T_f04aa_row1_col0\" class=\"data row1 col0\" >pytorch-yolox-object-detector/2023-08-17_16-14-43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f965015c5b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The name for the project\n",
    "project_name = f\"pytorch-yolox-object-detector\"\n",
    "\n",
    "# The path for the project folder\n",
    "project_dir = Path(f\"./{project_name}/\")\n",
    "\n",
    "# Create the project directory if it does not already exist\n",
    "project_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The path to the checkpoint folder\n",
    "checkpoint_dir = Path(project_dir/f\"2023-08-17_16-14-43\")\n",
    "# checkpoint_dir = Path(project_dir/f\"pretrained-coco\")\n",
    "\n",
    "pd.Series({\n",
    "    \"Project Directory:\": project_dir,\n",
    "    \"Checkpoint Directory:\": checkpoint_dir,\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a96378",
   "metadata": {},
   "source": [
    "### Download a Font File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37213df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./KFOlCnqEu92Fr1MmEU9vAw.ttf already exists and overwrite is set to False.\n"
     ]
    }
   ],
   "source": [
    "# Set the name of the font file\n",
    "font_file = 'KFOlCnqEu92Fr1MmEU9vAw.ttf'\n",
    "\n",
    "# Download the font file\n",
    "download_file(f\"https://fonts.gstatic.com/s/roboto/v30/{font_file}\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9abf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cd8dddc",
   "metadata": {},
   "source": [
    "## Loading the Checkpoint Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb7352",
   "metadata": {},
   "source": [
    "### Load the Colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3560146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The colormap path\n",
    "colormap_path = list(checkpoint_dir.glob('*colormap.json'))[0]\n",
    "\n",
    "# Load the JSON colormap data\n",
    "with open(colormap_path, 'r') as file:\n",
    "        colormap_json = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a dictionary        \n",
    "colormap_dict = {item['label']: item['color'] for item in colormap_json['items']}\n",
    "\n",
    "# Extract the class names from the colormap\n",
    "class_names = list(colormap_dict.keys())\n",
    "\n",
    "# Make a copy of the colormap in integer format\n",
    "int_colors = [tuple(int(c*255) for c in color) for color in colormap_dict.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50026c6f-237e-424c-a28c-1be6bc2a5340",
   "metadata": {},
   "source": [
    "### Set the Preprocessing and Post-Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a339e0-170f-47d4-867c-3d0b1e881c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stride = 32\n",
    "input_dim_slice = slice(2, 4, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c482311-1f64-41c4-9c55-e6847beda910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1abbfa9a-3316-4321-812d-4bfbdcb4533d",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb4eff-7472-49a9-be02-16fddc77bdeb",
   "metadata": {},
   "source": [
    "### Define a Function to Prepare Images for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac96b9e2-0ad3-45dc-9ca6-aeec68570eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_for_inference(frame, target_sz, max_stride):\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    # Resize image without cropping to multiple of the max stride\n",
    "    resized_img = resize_img(rgb_img, target_sz=target_sz, divisor=1)\n",
    "    \n",
    "    # Calculating the input dimensions that multiples of the max stride\n",
    "    input_dims = [dim - dim % max_stride for dim in resized_img.size]\n",
    "    # Calculate the offsets from the resized image dimensions to the input dimensions\n",
    "    offsets = (np.array(resized_img.size) - input_dims) / 2\n",
    "    # Calculate the scale between the source image and the resized image\n",
    "    min_img_scale = min(rgb_img.size) / min(resized_img.size)\n",
    "    \n",
    "    # Crop the resized image to the input dimensions\n",
    "    input_img = resized_img.crop(box=[*offsets, *resized_img.size - offsets])\n",
    "    \n",
    "    return rgb_img, input_dims, offsets, min_img_scale, input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b408dc-097e-47fa-962b-13e100aa02a2",
   "metadata": {},
   "source": [
    "### Define Functions to Process YOLOX Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff62779-8db0-49c1-8019-ce12864b704c",
   "metadata": {},
   "source": [
    "#### Define a function to generate the output grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b75909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_grids_np(height, width, strides=[8,16,32]):\n",
    "    \"\"\"\n",
    "    Generate a numpy array containing grid coordinates and strides for a given height and width.\n",
    "\n",
    "    Args:\n",
    "        height (int): The height of the image.\n",
    "        width (int): The width of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array containing grid coordinates and strides.\n",
    "    \"\"\"\n",
    "\n",
    "    all_coordinates = []\n",
    "\n",
    "    for stride in strides:\n",
    "        # Calculate the grid height and width\n",
    "        grid_height = height // stride\n",
    "        grid_width = width // stride\n",
    "\n",
    "        # Generate grid coordinates\n",
    "        g1, g0 = np.meshgrid(np.arange(grid_height), np.arange(grid_width), indexing='ij')\n",
    "\n",
    "        # Create an array of strides\n",
    "        s = np.full((grid_height, grid_width), stride)\n",
    "\n",
    "        # Stack the coordinates along with the stride\n",
    "        coordinates = np.stack((g0.flatten(), g1.flatten(), s.flatten()), axis=-1)\n",
    "\n",
    "        # Append to the list\n",
    "        all_coordinates.append(coordinates)\n",
    "\n",
    "    # Concatenate all arrays in the list along the first dimension\n",
    "    output_grids = np.concatenate(all_coordinates, axis=0)\n",
    "\n",
    "    return output_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292412d-1713-4621-aa1f-a9afebf47b8f",
   "metadata": {},
   "source": [
    "#### Define a function to calculate bounding boxes and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b471b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_boxes_and_probs(model_output:np.ndarray, output_grids:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the bounding boxes and their probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    model_output (numpy.ndarray): The output of the model.\n",
    "    output_grids (numpy.ndarray): The output grids.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The array containing the bounding box coordinates, class labels, and maximum probabilities.\n",
    "    \"\"\"\n",
    "    # Calculate the bounding box coordinates\n",
    "    box_centroids = (model_output[..., :2] + output_grids[..., :2]) * output_grids[..., 2:]\n",
    "    box_sizes = np.exp(model_output[..., 2:4]) * output_grids[..., 2:]\n",
    "\n",
    "    x0, y0 = [t.squeeze(axis=2) for t in np.split(box_centroids - box_sizes / 2, 2, axis=2)]\n",
    "    w, h = [t.squeeze(axis=2) for t in np.split(box_sizes, 2, axis=2)]\n",
    "\n",
    "    # Calculate the probabilities for each class\n",
    "    box_objectness = model_output[..., 4]\n",
    "    box_cls_scores = model_output[..., 5:]\n",
    "    box_probs = np.expand_dims(box_objectness, -1) * box_cls_scores\n",
    "\n",
    "    # Get the maximum probability and corresponding class for each proposal\n",
    "    max_probs = np.max(box_probs, axis=-1)\n",
    "    labels = np.argmax(box_probs, axis=-1)\n",
    "\n",
    "    return np.array([x0, y0, w, h, labels, max_probs]).transpose((1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e2c04-bd0e-44f2-9af1-750b4aeddae3",
   "metadata": {},
   "source": [
    "#### Define a function to extract object proposals from the raw model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e8ff234-038b-4238-9d87-308fb8553286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_outputs(outputs, input_dims, bbox_conf_thresh):\n",
    "    # Process the model output\n",
    "    outputs = calculate_boxes_and_probs(outputs, generate_output_grids_np(*input_dims))\n",
    "    # Filter the proposals based on the confidence threshold\n",
    "    max_probs = outputs[:, :, -1]\n",
    "    mask = max_probs > bbox_conf_thresh\n",
    "    proposals = outputs[mask]\n",
    "    # Sort the proposals by probability in descending order\n",
    "    proposals = proposals[proposals[..., -1].argsort()][::-1]\n",
    "    return proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa62660-a012-426a-b2e6-36dc58aa8d5d",
   "metadata": {},
   "source": [
    "#### Define a function to calculate the intersection-over-union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3cc1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(proposals:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) for all pairs of bounding boxes (x,y,w,h) in 'proposals'.\n",
    "\n",
    "    The IoU is a measure of overlap between two bounding boxes. It is calculated as the area of\n",
    "    intersection divided by the area of union of the two boxes.\n",
    "\n",
    "    Parameters:\n",
    "    proposals (2D np.array): A NumPy array of bounding boxes, where each box is an array [x, y, width, height].\n",
    "\n",
    "    Returns:\n",
    "    iou (2D np.array): The IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate coordinates for the intersection rectangles\n",
    "    x1 = np.maximum(proposals[:, 0], proposals[:, 0][:, None])\n",
    "    y1 = np.maximum(proposals[:, 1], proposals[:, 1][:, None])\n",
    "    x2 = np.minimum(proposals[:, 0] + proposals[:, 2], (proposals[:, 0] + proposals[:, 2])[:, None])\n",
    "    y2 = np.minimum(proposals[:, 1] + proposals[:, 3], (proposals[:, 1] + proposals[:, 3])[:, None])\n",
    "    \n",
    "    # Calculate intersection areas\n",
    "    intersections = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "\n",
    "    # Calculate union areas\n",
    "    areas = proposals[:, 2] * proposals[:, 3]\n",
    "    unions = areas[:, None] + areas - intersections\n",
    "\n",
    "    # Calculate IoUs\n",
    "    iou = intersections / unions\n",
    "\n",
    "    # Return the iou matrix\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690123dc-c9c6-4ea9-a503-b81780be53af",
   "metadata": {},
   "source": [
    "#### Define a function to filter bounding box proposals using Non-Maximum Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb3ca40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_sorted_boxes(iou:np.ndarray, iou_thresh:float=0.45) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies non-maximum suppression (NMS) to sorted bounding boxes.\n",
    "\n",
    "    It suppresses boxes that have high overlap (as defined by the IoU threshold) with a box that \n",
    "    has a higher score.\n",
    "\n",
    "    Parameters:\n",
    "    iou (np.ndarray): An IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    iou_thresh (float): The IoU threshold for suppression. Boxes with IoU > iou_thresh are suppressed.\n",
    "\n",
    "    Returns:\n",
    "    keep (np.ndarray): The indices of the boxes to keep after applying NMS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask to keep track of boxes\n",
    "    mask = np.ones(iou.shape[0], dtype=bool)\n",
    "\n",
    "    # Apply non-max suppression\n",
    "    for i in range(iou.shape[0]):\n",
    "        if mask[i]:\n",
    "            # Suppress boxes with higher index and IoU > threshold\n",
    "            mask[(iou[i] > iou_thresh) & (np.arange(iou.shape[0]) > i)] = False\n",
    "\n",
    "    # Return the indices of the boxes to keep\n",
    "    return np.arange(iou.shape[0])[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca8d66-8c93-4029-87ad-8d9ccdf35bdb",
   "metadata": {},
   "source": [
    "### Define a Function to Annotate Images with Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f1ea481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes_pil(image, boxes, labels, colors, font, width:int=2, font_size:int=18, probs=None):\n",
    "    \"\"\"\n",
    "    Annotates an image with bounding boxes, labels, and optional probability scores.\n",
    "\n",
    "    This function draws bounding boxes on the provided image using the given box coordinates, \n",
    "    colors, and labels. If probabilities are provided, they will be added to the labels.\n",
    "\n",
    "    Parameters:\n",
    "    image (PIL.Image): The input image on which annotations will be drawn.\n",
    "    boxes (list of tuples): A list of bounding box coordinates where each tuple is (x, y, w, h).\n",
    "    labels (list of str): A list of labels corresponding to each bounding box.\n",
    "    colors (list of str): A list of colors for each bounding box and its corresponding label.\n",
    "    font (str): Path to the font file to be used for displaying the labels.\n",
    "    width (int, optional): Width of the bounding box lines. Defaults to 2.\n",
    "    font_size (int, optional): Size of the font for the labels. Defaults to 25.\n",
    "    probs (list of float, optional): A list of probability scores corresponding to each label. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    annotated_image (PIL.Image): The image annotated with bounding boxes, labels, and optional probability scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a reference diagonal\n",
    "    REFERENCE_DIAGONAL = 1000\n",
    "    \n",
    "    # Scale the font size using the hypotenuse of the image\n",
    "    font_size = int(font_size * (np.hypot(*image.size) / REFERENCE_DIAGONAL))\n",
    "    \n",
    "    # Add probability scores to labels\n",
    "    if probs is not None:\n",
    "        labels = [f\"{label}: {prob*100:.2f}%\" for label, prob in zip(labels, probs)]\n",
    "    \n",
    "    # Create a copy of the image\n",
    "    annotated_image = image.copy()\n",
    "\n",
    "    # Create an ImageDraw object for drawing on the image\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "    # Loop through the bounding boxes and labels in the 'annotation' DataFrame\n",
    "    for i in range(len(labels)):\n",
    "        # Get the bounding box coordinates\n",
    "        x, y, w, h = boxes[i]\n",
    "\n",
    "        # Create a tuple of coordinates for the bounding box\n",
    "        shape = (x, y, x+w, y+h)\n",
    "\n",
    "        # Draw the bounding box on the image\n",
    "        draw.rectangle(shape, outline=colors[i], width=width)\n",
    "        \n",
    "        # Load the font file\n",
    "        fnt = ImageFont.truetype(font, font_size)\n",
    "        \n",
    "        # Draw the label box on the image\n",
    "        label_w, label_h = draw.textbbox(xy=(0,0), text=labels[i], font=fnt)[2:]\n",
    "        draw.rectangle((x, y-label_h, x+label_w, y), outline=colors[i], fill=colors[i], width=width)\n",
    "\n",
    "        # Draw the label on the image\n",
    "        draw.multiline_text((x, y-label_h), labels[i], font=fnt, fill='black' if np.mean(colors[i]) > 127.5 else 'white')\n",
    "        \n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa378c-f6e9-4c8c-9521-50ffe4554ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fffabb13",
   "metadata": {},
   "source": [
    "## Performing Inference with ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837f4dd",
   "metadata": {},
   "source": [
    "### Create an Inference Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2eeb4ef-1b2c-4031-be5f-87e45b4c8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a filename for the ONNX model\n",
    "onnx_file_path = list(checkpoint_dir.glob('*.onnx'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "025f7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and create an InferenceSession\n",
    "providers = [\n",
    "    'CPUExecutionProvider',\n",
    "    # \"CUDAExecutionProvider\",\n",
    "]\n",
    "sess_options = ort.SessionOptions()\n",
    "session = ort.InferenceSession(onnx_file_path, sess_options=sess_options, providers=providers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f263602-1d53-45d7-9938-6748b89ca802",
   "metadata": {},
   "source": [
    "### Select a Test Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a17914c-e22c-4155-93fd-68b3a050375d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./videos/pexels-rodnae-productions-10373924.mp4 already exists and overwrite is set to False.\n"
     ]
    }
   ],
   "source": [
    "video_dir = \"./videos/\"\n",
    "test_video_name = \"pexels-rodnae-productions-10373924.mp4\"\n",
    "# test_video_name = \"cars_on_highway.mp4\"\n",
    "video_path = f\"{video_dir}{test_video_name}\"\n",
    "\n",
    "test_video_url = f\"https://huggingface.co/datasets/cj-mills/pexels-object-tracking-test-videos/resolve/main/{test_video_name}\"\n",
    "\n",
    "download_file(test_video_url, video_dir, False)\n",
    "\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45c032-3aca-44c3-ab38-164f74ad0973",
   "metadata": {},
   "source": [
    "### Initialize a `VideoCapture` Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d2f96c2-d5b2-4749-8ff5-8edf4e55bfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_811b0\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_811b0_level0_row0\" class=\"row_heading level0 row0\" >Frame Width:</th>\n",
       "      <td id=\"T_811b0_row0_col0\" class=\"data row0 col0\" >1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_811b0_level0_row1\" class=\"row_heading level0 row1\" >Frame Height:</th>\n",
       "      <td id=\"T_811b0_row1_col0\" class=\"data row1 col0\" >1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_811b0_level0_row2\" class=\"row_heading level0 row2\" >Frame FPS:</th>\n",
       "      <td id=\"T_811b0_row2_col0\" class=\"data row2 col0\" >29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_811b0_level0_row3\" class=\"row_heading level0 row3\" >Frames:</th>\n",
       "      <td id=\"T_811b0_row3_col0\" class=\"data row3 col0\" >226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f951c3e19f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_width = int(video_capture.get(3))\n",
    "frame_height = int(video_capture.get(4))\n",
    "frame_fps = int(video_capture.get(5))\n",
    "frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "pd.Series({\n",
    "    \"Frame Width:\": frame_width,\n",
    "    \"Frame Height:\": frame_height,\n",
    "    \"Frame FPS:\": frame_fps,\n",
    "    \"Frames:\": frames\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39151259-1b0b-4b02-8c2d-bd0793578acb",
   "metadata": {},
   "source": [
    "### Initialize a `VideoWriter` Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb67d2b7-8801-47fa-8e8c-12a02a00a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_out_path = f\"{(video_dir)}{Path(video_path).stem}-deep-sort.mp4\"\n",
    "video_writer = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'mp4v'), frame_fps, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c3413-ae8e-4697-a528-36f6cd1bf456",
   "metadata": {},
   "source": [
    "### Define Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7deefea6-baeb-4414-bfab-16cce20cec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sz = 288\n",
    "# test_sz = 384\n",
    "bbox_conf_thresh = 0.5\n",
    "iou_thresh = 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee3a8e-472e-4bc8-952c-480cc156be57",
   "metadata": {},
   "source": [
    "### Initialize a Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed29ba66-9648-4b61-99bd-1a4c49307b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mobilenet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>torchreid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clip_RN50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clip_RN101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clip_RN50x4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>clip_RN50x16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>clip_ViT-B/32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clip_ViT-B/16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0      mobilenet\n",
       "1      torchreid\n",
       "2      clip_RN50\n",
       "3     clip_RN101\n",
       "4    clip_RN50x4\n",
       "5   clip_RN50x16\n",
       "6  clip_ViT-B/32\n",
       "7  clip_ViT-B/16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(EMBEDDER_CHOICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7c2485c-37c4-4dfc-832a-f70a9adc16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = DeepSort(max_age=15, embedder=EMBEDDER_CHOICES[0], half=True, embedder_gpu=True, bgr=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee0516-2c33-451a-b04d-6c6a8fbdd203",
   "metadata": {},
   "source": [
    "### Detect, Track, and Annotate Objects in Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "432eb4b2-cd60-422c-9dd5-1f622ad31ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7b711f457947d496c72ee4f3485ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tracker.delete_all_tracks()\n",
    "\n",
    "with tqdm(total=frames, desc=\"Processing frames\") as pbar:\n",
    "    while video_capture.isOpened():\n",
    "        ret, frame = video_capture.read()\n",
    "        if ret:\n",
    "\n",
    "            # Prepare an input image for inference\n",
    "            rgb_img, input_dims, offsets, min_img_scale, input_img = prepare_image_for_inference(frame, test_sz, max_stride)\n",
    "                        \n",
    "            # Convert the existing input image to NumPy format\n",
    "            input_tensor_np = np.array(input_img, dtype=np.float32).transpose((2, 0, 1))[None]/255\n",
    "\n",
    "            # Start performance counter\n",
    "            start_time = time.perf_counter()\n",
    "                        \n",
    "            # Run inference\n",
    "            outputs = session.run(None, {\"input\": input_tensor_np})[0]\n",
    "\n",
    "            # Process the model output\n",
    "            proposals = process_outputs(outputs, input_tensor_np.shape[input_dim_slice], bbox_conf_thresh)\n",
    "            \n",
    "            # Apply non-max suppression to the proposals with the specified threshold\n",
    "            proposal_indices = nms_sorted_boxes(calc_iou(proposals[:, :-2]), iou_thresh)\n",
    "            proposals = proposals[proposal_indices]\n",
    "            \n",
    "            bbox_list = proposals[:,:4]\n",
    "            # bbox_list = (proposals[:,:4]+[*offsets, 0, 0])*min_img_scale\n",
    "            label_list = [class_names[int(idx)] for idx in proposals[:,4]]\n",
    "            probs_list = proposals[:,5]\n",
    "\n",
    "            detections = [(box, prob, label) for box, prob, label in zip(bbox_list, probs_list, label_list)]\n",
    "            \n",
    "            # Update tracker with detections.\n",
    "            tracks = tracker.update_tracks(detections, frame=np.array(input_img))\n",
    "\n",
    "            # End performance counter\n",
    "            end_time = time.perf_counter()\n",
    "            # Calculate the combined FPS for object detection and tracking\n",
    "            fps = 1 / (end_time - start_time)\n",
    "            # Display the frame rate in the progress bar\n",
    "            pbar.set_postfix(fps=fps)\n",
    "            \n",
    "            bbox_list = (np.array([track.to_tlwh() for track in tracks])+[*offsets, 0, 0])*min_img_scale\n",
    "            label_list = [track.det_class for track in tracks]\n",
    "            probs_list = [track.det_conf if track.det_conf != None else 0 for track in tracks]\n",
    "\n",
    "            # Annotate the current frame with bounding boxes and tracking IDs\n",
    "            annotated_img = draw_bboxes_pil(\n",
    "                image=rgb_img, \n",
    "                boxes=bbox_list, \n",
    "                labels=[f\"{track.track_id}-{track.det_class}\" for track in tracks],\n",
    "                probs=probs_list,\n",
    "                colors=[int_colors[class_names.index(i)] for i in label_list], \n",
    "                font=font_file\n",
    "            )\n",
    "            annotated_frame = cv2.cvtColor(np.array(annotated_img), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            video_writer.write(annotated_frame)\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            break\n",
    "video_capture.release()\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea67d03-5f1c-4afe-9229-95d374c901db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
