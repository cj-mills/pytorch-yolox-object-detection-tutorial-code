{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f6547b-53ff-4f10-988d-caa7ad1a7deb",
   "metadata": {},
   "source": [
    "## Importing the Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32799c3-236a-4b86-8f35-24065c4ea5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json  # For JSON data handling\n",
    "from pathlib import Path  # For file path operations\n",
    "import time  # For time-related functions\n",
    "import threading  # For multi-threading support\n",
    "from typing import List  # For type hinting\n",
    "import queue  # For queue data structure\n",
    "\n",
    "# ByteTrack package for object tracking\n",
    "from cjm_byte_track.core import BYTETracker\n",
    "from cjm_byte_track.matching import match_detections_with_tracks\n",
    "\n",
    "# Utility functions\n",
    "from cjm_psl_utils.core import download_file  # For downloading files\n",
    "from cjm_pil_utils.core import resize_img  # For resizing images\n",
    "\n",
    "# OpenCV for computer vision tasks\n",
    "import cv2\n",
    "\n",
    "# NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# PIL (Python Imaging Library) for image processing\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# ONNX (Open Neural Network Exchange) for machine learning interoperability\n",
    "import onnxruntime as ort  # ONNX Runtime for model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e8666-b69a-416e-8d4b-55c363e52b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5caad8af-ef11-406c-b4f4-dea0923928ca",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f059e0-7383-449f-af83-b434a54a63f4",
   "metadata": {},
   "source": [
    "### Define Functions for YOLOX Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74156398-5489-48a5-868f-4ab2419c7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_for_inference(frame:np.ndarray, target_sz:int, max_stride:int):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepares an image for inference by performing a series of preprocessing steps.\n",
    "    \n",
    "    Steps:\n",
    "    1. Converts a BGR image to RGB.\n",
    "    2. Resizes the image to a target size without cropping, considering a given divisor.\n",
    "    3. Calculates input dimensions as multiples of the max stride.\n",
    "    4. Calculates offsets based on the resized image dimensions and input dimensions.\n",
    "    5. Computes the scale between the original and resized image.\n",
    "    6. Crops the resized image based on calculated input dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    - frame (numpy.ndarray): The input image in BGR format.\n",
    "    - target_sz (int): The target minimum size for resizing the image.\n",
    "    - max_stride (int): The maximum stride to be considered for calculating input dimensions.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: \n",
    "    - rgb_img (PIL.Image): The converted RGB image.\n",
    "    - input_dims (list of int): Dimensions of the image that are multiples of max_stride.\n",
    "    - offsets (numpy.ndarray): Offsets from the resized image dimensions to the input dimensions.\n",
    "    - min_img_scale (float): Scale factor between the original and resized image.\n",
    "    - input_img (PIL.Image): Cropped image based on the calculated input dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    # Resize image without cropping to multiple of the max stride\n",
    "    resized_img = resize_img(rgb_img, target_sz=target_sz, divisor=1)\n",
    "    \n",
    "    # Calculating the input dimensions that multiples of the max stride\n",
    "    input_dims = [dim - dim % max_stride for dim in resized_img.size]\n",
    "    # Calculate the offsets from the resized image dimensions to the input dimensions\n",
    "    offsets = (np.array(resized_img.size) - input_dims) / 2\n",
    "    # Calculate the scale between the source image and the resized image\n",
    "    min_img_scale = min(rgb_img.size) / min(resized_img.size)\n",
    "    \n",
    "    # Crop the resized image to the input dimensions\n",
    "    input_img = resized_img.crop(box=[*offsets, *resized_img.size - offsets])\n",
    "    \n",
    "    return rgb_img, input_dims, offsets, min_img_scale, input_img\n",
    "\n",
    "def generate_output_grids_np(height, width, strides=[8,16,32]):\n",
    "    \"\"\"\n",
    "    Generate a numpy array containing grid coordinates and strides for a given height and width.\n",
    "\n",
    "    Args:\n",
    "        height (int): The height of the image.\n",
    "        width (int): The width of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array containing grid coordinates and strides.\n",
    "    \"\"\"\n",
    "\n",
    "    all_coordinates = []\n",
    "\n",
    "    for stride in strides:\n",
    "        # Calculate the grid height and width\n",
    "        grid_height = height // stride\n",
    "        grid_width = width // stride\n",
    "\n",
    "        # Generate grid coordinates\n",
    "        g1, g0 = np.meshgrid(np.arange(grid_height), np.arange(grid_width), indexing='ij')\n",
    "\n",
    "        # Create an array of strides\n",
    "        s = np.full((grid_height, grid_width), stride)\n",
    "\n",
    "        # Stack the coordinates along with the stride\n",
    "        coordinates = np.stack((g0.flatten(), g1.flatten(), s.flatten()), axis=-1)\n",
    "\n",
    "        # Append to the list\n",
    "        all_coordinates.append(coordinates)\n",
    "\n",
    "    # Concatenate all arrays in the list along the first dimension\n",
    "    output_grids = np.concatenate(all_coordinates, axis=0)\n",
    "\n",
    "    return output_grids\n",
    "\n",
    "def calculate_boxes_and_probs(model_output:np.ndarray, output_grids:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the bounding boxes and their probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    model_output (numpy.ndarray): The output of the model.\n",
    "    output_grids (numpy.ndarray): The output grids.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The array containing the bounding box coordinates, class labels, and maximum probabilities.\n",
    "    \"\"\"\n",
    "    # Calculate the bounding box coordinates\n",
    "    box_centroids = (model_output[..., :2] + output_grids[..., :2]) * output_grids[..., 2:]\n",
    "    box_sizes = np.exp(model_output[..., 2:4]) * output_grids[..., 2:]\n",
    "\n",
    "    x0, y0 = [t.squeeze(axis=2) for t in np.split(box_centroids - box_sizes / 2, 2, axis=2)]\n",
    "    w, h = [t.squeeze(axis=2) for t in np.split(box_sizes, 2, axis=2)]\n",
    "\n",
    "    # Calculate the probabilities for each class\n",
    "    box_objectness = model_output[..., 4]\n",
    "    box_cls_scores = model_output[..., 5:]\n",
    "    box_probs = np.expand_dims(box_objectness, -1) * box_cls_scores\n",
    "\n",
    "    # Get the maximum probability and corresponding class for each proposal\n",
    "    max_probs = np.max(box_probs, axis=-1)\n",
    "    labels = np.argmax(box_probs, axis=-1)\n",
    "\n",
    "    return np.array([x0, y0, w, h, labels, max_probs]).transpose((1, 2, 0))\n",
    "\n",
    "def process_outputs(outputs:np.ndarray, input_dims:tuple, bbox_conf_thresh:float):\n",
    "\n",
    "    \"\"\"\n",
    "    Process the model outputs to generate bounding box proposals filtered by confidence threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - outputs (numpy.ndarray): The raw output from the model, which will be processed to calculate boxes and probabilities.\n",
    "    - input_dims (tuple of int): Dimensions (height, width) of the input image to the model.\n",
    "    - bbox_conf_thresh (float): Threshold for the bounding box confidence/probability. Bounding boxes with a confidence\n",
    "                                score below this threshold will be discarded.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.array: An array of proposals where each proposal is an array containing bounding box coordinates\n",
    "                   and its associated probability, sorted in descending order by probability.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the model output\n",
    "    outputs = calculate_boxes_and_probs(outputs, generate_output_grids_np(*input_dims))\n",
    "    # Filter the proposals based on the confidence threshold\n",
    "    max_probs = outputs[:, :, -1]\n",
    "    mask = max_probs > bbox_conf_thresh\n",
    "    proposals = outputs[mask]\n",
    "    # Sort the proposals by probability in descending order\n",
    "    proposals = proposals[proposals[..., -1].argsort()][::-1]\n",
    "    return proposals\n",
    "\n",
    "def calc_iou(proposals:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) for all pairs of bounding boxes (x,y,w,h) in 'proposals'.\n",
    "\n",
    "    The IoU is a measure of overlap between two bounding boxes. It is calculated as the area of\n",
    "    intersection divided by the area of union of the two boxes.\n",
    "\n",
    "    Parameters:\n",
    "    proposals (2D np.array): A NumPy array of bounding boxes, where each box is an array [x, y, width, height].\n",
    "\n",
    "    Returns:\n",
    "    iou (2D np.array): The IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate coordinates for the intersection rectangles\n",
    "    x1 = np.maximum(proposals[:, 0], proposals[:, 0][:, None])\n",
    "    y1 = np.maximum(proposals[:, 1], proposals[:, 1][:, None])\n",
    "    x2 = np.minimum(proposals[:, 0] + proposals[:, 2], (proposals[:, 0] + proposals[:, 2])[:, None])\n",
    "    y2 = np.minimum(proposals[:, 1] + proposals[:, 3], (proposals[:, 1] + proposals[:, 3])[:, None])\n",
    "    \n",
    "    # Calculate intersection areas\n",
    "    intersections = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "\n",
    "    # Calculate union areas\n",
    "    areas = proposals[:, 2] * proposals[:, 3]\n",
    "    unions = areas[:, None] + areas - intersections\n",
    "\n",
    "    # Calculate IoUs\n",
    "    iou = intersections / unions\n",
    "\n",
    "    # Return the iou matrix\n",
    "    return iou\n",
    "\n",
    "def nms_sorted_boxes(iou:np.ndarray, iou_thresh:float=0.45) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies non-maximum suppression (NMS) to sorted bounding boxes.\n",
    "\n",
    "    It suppresses boxes that have high overlap (as defined by the IoU threshold) with a box that \n",
    "    has a higher score.\n",
    "\n",
    "    Parameters:\n",
    "    iou (np.ndarray): An IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    iou_thresh (float): The IoU threshold for suppression. Boxes with IoU > iou_thresh are suppressed.\n",
    "\n",
    "    Returns:\n",
    "    keep (np.ndarray): The indices of the boxes to keep after applying NMS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask to keep track of boxes\n",
    "    mask = np.ones(iou.shape[0], dtype=bool)\n",
    "\n",
    "    # Apply non-max suppression\n",
    "    for i in range(iou.shape[0]):\n",
    "        if mask[i]:\n",
    "            # Suppress boxes with higher index and IoU > threshold\n",
    "            mask[(iou[i] > iou_thresh) & (np.arange(iou.shape[0]) > i)] = False\n",
    "\n",
    "    # Return the indices of the boxes to keep\n",
    "    return np.arange(iou.shape[0])[mask]\n",
    "\n",
    "def draw_bboxes_pil(image, boxes, labels, colors, font, width=2, font_size=18, probs=None):\n",
    "    \"\"\"\n",
    "    Annotates an image with bounding boxes, labels, and optional probability scores.\n",
    "\n",
    "    Parameters:\n",
    "    - image (PIL.Image): The input image on which annotations will be drawn.\n",
    "    - boxes (list of tuples): A list of bounding box coordinates where each tuple is (x, y, w, h).\n",
    "    - labels (list of str): A list of labels corresponding to each bounding box.\n",
    "    - colors (list of str): A list of colors for each bounding box and its corresponding label.\n",
    "    - font (str): Path to the font file to be used for displaying the labels.\n",
    "    - width (int, optional): Width of the bounding box lines. Defaults to 2.\n",
    "    - font_size (int, optional): Size of the font for the labels. Defaults to 18.\n",
    "    - probs (list of float, optional): A list of probability scores corresponding to each label. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    - annotated_image (PIL.Image): The image annotated with bounding boxes, labels, and optional probability scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a reference diagonal\n",
    "    REFERENCE_DIAGONAL = 1000\n",
    "    \n",
    "    # Scale the font size using the hypotenuse of the image\n",
    "    font_size = int(font_size * (np.hypot(*image.size) / REFERENCE_DIAGONAL))\n",
    "    \n",
    "    # Add probability scores to labels if provided\n",
    "    if probs is not None:\n",
    "        labels = [f\"{label}: {prob*100:.2f}%\" for label, prob in zip(labels, probs)]\n",
    "\n",
    "    # Create an ImageDraw object for drawing on the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load the font file (outside the loop)\n",
    "    fnt = ImageFont.truetype(font, font_size)\n",
    "    \n",
    "    # Compute the mean color value for each color\n",
    "    mean_colors = [np.mean(np.array(color)) for color in colors]\n",
    "\n",
    "    # Loop through the bounding boxes, labels, and colors\n",
    "    for box, label, color, mean_color in zip(boxes, labels, colors, mean_colors):\n",
    "        # Get the bounding box coordinates\n",
    "        x, y, w, h = box\n",
    "\n",
    "        # Draw the bounding box on the image\n",
    "        draw.rectangle([x, y, x+w, y+h], outline=color, width=width)\n",
    "        \n",
    "        # Get the size of the label text box\n",
    "        label_w, label_h = draw.textbbox(xy=(0,0), text=label, font=fnt)[2:]\n",
    "        \n",
    "        # Draw the label rectangle on the image\n",
    "        draw.rectangle([x, y-label_h, x+label_w, y], outline=color, fill=color)\n",
    "\n",
    "        # Draw the label text on the image\n",
    "        font_color = 'black' if mean_color > 127.5 else 'white'\n",
    "        draw.multiline_text((x, y-label_h), label, font=fnt, fill=font_color)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3f8e6-ecdb-4956-a349-23aa00fa45b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9747f0ab-cf7b-4225-bfa0-bc44195ce517",
   "metadata": {},
   "source": [
    "### Define a Function to Generate a GStreamer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557d5d8d-a2d5-4275-a596-5e2c960e23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gstreamer_pipeline(\n",
    "    sensor_id=0,\n",
    "    capture_width=1920,\n",
    "    capture_height=1080,\n",
    "    display_width=960,\n",
    "    display_height=540,\n",
    "    framerate=30,\n",
    "    flip_method=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a GStreamer pipeline string for capturing and processing video from a camera.\n",
    "\n",
    "    This function creates a pipeline that captures video from an NVIDIA Argus camera,\n",
    "    performs necessary conversions, and prepares the video for display or further processing.\n",
    "\n",
    "    Args:\n",
    "        sensor_id (int): The ID of the camera sensor to use (default: 0).\n",
    "        capture_width (int): The width of the captured video in pixels (default: 1920).\n",
    "        capture_height (int): The height of the captured video in pixels (default: 1080).\n",
    "        display_width (int): The width of the displayed/processed video in pixels (default: 960).\n",
    "        display_height (int): The height of the displayed/processed video in pixels (default: 540).\n",
    "        framerate (int): The desired framerate of the video capture (default: 30).\n",
    "        flip_method (int): The method used to flip the image, if needed (default: 0, no flip).\n",
    "\n",
    "    Returns:\n",
    "        str: A GStreamer pipeline string that can be used with GStreamer-based applications.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        # Start with nvarguscamerasrc to capture from NVIDIA Argus camera\n",
    "        f\"nvarguscamerasrc sensor-id={sensor_id} ! \"\n",
    "        # Set the captured video format and properties\n",
    "        f\"video/x-raw(memory:NVMM), width=(int){capture_width}, height=(int){capture_height}, framerate=(fraction){framerate}/1 ! \"\n",
    "        # Use nvvidconv to convert the video and potentially flip the image\n",
    "        f\"nvvidconv flip-method={flip_method} ! \"\n",
    "        # Set the display/processing video format and properties\n",
    "        f\"video/x-raw, width=(int){display_width}, height=(int){display_height}, format=(string)BGRx ! \"\n",
    "        # Convert the video color format\n",
    "        f\"videoconvert ! \"\n",
    "        # Set the final video format to BGR for compatibility with OpenCV\n",
    "        f\"video/x-raw, format=(string)BGR ! appsink\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822bf1f-b291-4dff-9f18-4d224ffa9423",
   "metadata": {},
   "source": [
    "### Define a Wrapper Class for Reading Camera Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbb336b7-c19a-4924-b359-77503d41a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDropper:\n",
    "    \"\"\"\n",
    "    A class for efficiently reading frames from a video capture device,\n",
    "    dropping frames if necessary to maintain real-time processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cv2_capture: cv2.VideoCapture, queue_size=1):\n",
    "        \"\"\"\n",
    "        Initialize the FrameDropper.\n",
    "\n",
    "        Args:\n",
    "            cv2_capture (cv2.VideoCapture): The video capture object.\n",
    "            queue_size (int): Maximum number of frames to keep in the queue.\n",
    "        \"\"\"\n",
    "        # Store the video capture object\n",
    "        self.cap = cv2_capture\n",
    "        \n",
    "        # Create a queue to store frames with a maximum size\n",
    "        self.q = queue.Queue(maxsize=queue_size)\n",
    "        \n",
    "        # Create an event to signal when to stop the thread\n",
    "        self.stop_flag = threading.Event()\n",
    "        \n",
    "        # Create a separate thread for reading frames\n",
    "        self.thread = threading.Thread(target=self._reader)\n",
    "        \n",
    "        # Set the thread as a daemon, so it will automatically close when the main program exits\n",
    "        self.thread.daemon = True\n",
    "        \n",
    "        # Start the thread\n",
    "        self.thread.start()\n",
    "\n",
    "    def _reader(self):\n",
    "        \"\"\"\n",
    "        Continuously read frames from the video capture device and manage the frame queue.\n",
    "        Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while not self.stop_flag.is_set():  # Continue until the stop flag is set\n",
    "            # Read a frame from the video capture device\n",
    "            ret, frame = self.cap.read()\n",
    "            \n",
    "            if not ret:  # If reading the frame failed, exit the loop\n",
    "                break\n",
    "            \n",
    "            if not self.q.full():  # If the queue is not full\n",
    "                self.q.put(frame)  # Add the frame to the queue\n",
    "            else:\n",
    "                try:\n",
    "                    # If the queue is full, try to remove the oldest frame\n",
    "                    self.q.get_nowait()\n",
    "                except queue.Empty:\n",
    "                    # If the queue is empty (unlikely, but possible due to race conditions)\n",
    "                    pass\n",
    "                # Add the new frame to the queue\n",
    "                self.q.put(frame)\n",
    "\n",
    "    def read(self):\n",
    "        \"\"\"\n",
    "        Read a frame from the queue.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (True, frame) where frame is the next available frame.\n",
    "        \"\"\"\n",
    "        # Get the next frame from the queue and return it\n",
    "        # The 'True' indicates that a frame was successfully read\n",
    "        return True, self.q.get()\n",
    "\n",
    "    def release(self):\n",
    "        \"\"\"\n",
    "        Stop the reading thread and release the video capture resources.\n",
    "        \"\"\"\n",
    "        # Set the stop flag to signal the thread to stop\n",
    "        self.stop_flag.set()\n",
    "        \n",
    "        # Release the video capture object\n",
    "        self.cap.release()\n",
    "        \n",
    "        # Wait for the thread to finish\n",
    "        self.thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378bfd0-8768-4103-a79e-fe991f1b3269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d1c55e-6a5f-4eec-8575-1dbda0a042dd",
   "metadata": {},
   "source": [
    "## Setting Up the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57214613-97c7-4f10-a068-1afbac5ee299",
   "metadata": {},
   "source": [
    "### Set the Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c5abfb6-39f3-4e11-840b-11dd61a53a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name for the project\n",
    "project_name = f\"pytorch-yolox-object-detector\"\n",
    "\n",
    "# The path for the project folder\n",
    "project_dir = Path(f\"./{project_name}/\")\n",
    "\n",
    "# Create the project directory if it does not already exist\n",
    "project_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The path to the checkpoint folder\n",
    "checkpoint_folder = \"2024-02-17_11-08-46\"\n",
    "checkpoint_dir = Path(project_dir/checkpoint_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bd851-6688-4eb8-8098-55cc56799fea",
   "metadata": {},
   "source": [
    "### Download a Font File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10cce57c-a7b4-466b-8eb6-c6001b62a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./KFOlCnqEu92Fr1MmEU9vAw.ttf already exists and overwrite is set to False.\n"
     ]
    }
   ],
   "source": [
    "# Set the name of the font file\n",
    "font_file = 'KFOlCnqEu92Fr1MmEU9vAw.ttf'\n",
    "\n",
    "# Download the font file\n",
    "download_file(f\"https://fonts.gstatic.com/s/roboto/v30/{font_file}\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84b9e4-7f43-45c7-9fc9-dbe6955d33d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf18169b-cb5f-4555-b23b-4a800016a7b4",
   "metadata": {},
   "source": [
    "## Loading the Checkpoint Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4ea176-0b1c-4f7b-a06f-a1cc474fcca4",
   "metadata": {},
   "source": [
    "### Load the Colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a59d3558-2755-4102-8d74-5ee9852bb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The colormap path\n",
    "colormap_path = list(checkpoint_dir.glob('*colormap.json'))[0]\n",
    "\n",
    "# Load the JSON colormap data\n",
    "with open(colormap_path, 'r') as file:\n",
    "        colormap_json = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a dictionary        \n",
    "colormap_dict = {item['label']: item['color'] for item in colormap_json['items']}\n",
    "\n",
    "# Extract the class names from the colormap\n",
    "class_names = list(colormap_dict.keys())\n",
    "\n",
    "# Make a copy of the colormap in integer format\n",
    "int_colors = [tuple(int(c*255) for c in color) for color in colormap_dict.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f6573-8211-4ed0-8010-afd174b63c82",
   "metadata": {},
   "source": [
    "### Set the Preprocessing and Post-Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f921025-ff92-40c3-866c-9ad6ba9e0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stride = 32\n",
    "input_dim_slice = slice(2, 4, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50ae61-7576-4b24-ba19-05f42e956502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4034a8f-3db9-4086-acfe-01d4b29bce33",
   "metadata": {},
   "source": [
    "### Create an Inference Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d4a0c70-70e8-4768-89c1-71e5e571793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filename for the ONNX model\n",
    "# Assumes there's only one .onnx file in the checkpoint directory\n",
    "onnx_file_path = list(checkpoint_dir.glob('*.onnx'))[0]\n",
    "\n",
    "# Set up a directory for TensorRT engine cache\n",
    "trt_cache_dir = checkpoint_dir / 'trt_engine_cache'\n",
    "\n",
    "# Initialize ONNX Runtime session options\n",
    "sess_opt = ort.SessionOptions()\n",
    "# Disable memory optimizations to potentially improve performance\n",
    "sess_opt.enable_cpu_mem_arena = False\n",
    "sess_opt.enable_mem_pattern = False\n",
    "sess_opt.enable_mem_reuse = False\n",
    "# Set execution mode to sequential for predictable behavior\n",
    "sess_opt.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "# Enable all graph optimizations\n",
    "sess_opt.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "# Configure TensorRT Execution Provider settings\n",
    "providers = [\n",
    "    ('TensorrtExecutionProvider', {\n",
    "        'device_id': 0,  # GPU device ID (0 for the first GPU)\n",
    "        'trt_int8_enable': True,  # Enable INT8 precision mode\n",
    "        'trt_engine_cache_enable': True,  # Enable caching of TensorRT engines\n",
    "        'trt_engine_cache_path': str(trt_cache_dir),  # Path to store TensorRT cache\n",
    "        'trt_int8_calibration_table_name': 'calibration.flatbuffers',  # INT8 calibration file\n",
    "        'trt_max_workspace_size': 4e9,  # Maximum TensorRT workspace size (4GB)\n",
    "        'trt_timing_cache_enable': True,  # Enable timing cache for faster engine building\n",
    "        'trt_force_sequential_engine_build': True,  # Build engines sequentially\n",
    "        'trt_dla_enable': False,  # Disable DLA (Deep Learning Accelerator)\n",
    "        'trt_max_partition_iterations': 1000,  # Max iterations for partitioning\n",
    "        'trt_min_subgraph_size': 1,  # Minimum subgraph size for TensorRT\n",
    "    })\n",
    "]\n",
    "\n",
    "# Create an ONNX Runtime InferenceSession with the specified options and providers\n",
    "session = ort.InferenceSession(onnx_file_path, sess_options=sess_opt, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d7cb9-2734-46fa-ac48-b238ee097cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0f91f7f-4e2e-429b-acf5-8df29c355d5b",
   "metadata": {},
   "source": [
    "### Define Camera Feed Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61a34fcb-c2f8-4a72-b894-80079428bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_csi = True\n",
    "sensor_id=0\n",
    "flip_method = 0\n",
    "framerate=60\n",
    "capture_width=1280\n",
    "capture_height=720"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ec1b8-11e7-4365-b9d3-a3249be2378c",
   "metadata": {},
   "source": [
    "### Define Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b585f69-805c-4020-a812-f435a108c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sz = 384\n",
    "bbox_conf_thresh = 0.35\n",
    "iou_thresh = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639fe496-bdd4-453f-b12d-36659d1db625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eca9011-34af-4a49-9234-a379f86ff5d1",
   "metadata": {},
   "source": [
    "### Build TensorRT Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7a6810a-c22f-4091-a760-2a1deb44fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-09-06 18:07:39.281744309 [W:onnxruntime:Default, tensorrt_execution_provider.cc:229 loadTimingCacheFile] [TensorRT EP] Could not read timing cache from: pytorch-yolox-object-detector/2024-02-17_11-08-46/trt_engine_cache/TensorrtExecutionProvider_cache_sm89.timing. A new timing cache will be generated and written.\u001b[m\n",
      "\u001b[0;93m2024-09-06 18:07:39.287246055 [W:onnxruntime:Default, tensorrt_execution_provider.h:90 log] [2024-09-07 01:07:39 WARNING] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 7.42 s, total: 1min 38s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a sample input with the target dimensions \n",
    "test_img = Image.fromarray(np.random.randn(capture_height, capture_width, 3).astype(np.uint8))\n",
    "resized_img = resize_img(test_img, test_sz)\n",
    "input_tensor_np = np.array(resized_img, dtype=np.float32).transpose((2, 0, 1))[None]/255\n",
    "\n",
    "# Perform a single inference run to build the TensorRT engine for the current input dimensions\n",
    "session.run(None, {\"input\": input_tensor_np});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca937b-0e9c-49bc-b300-1d035ad6d13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c8e23ab-ae94-4321-b9f7-ac5bc17e4c81",
   "metadata": {},
   "source": [
    "## Tracking Objects in a Camera Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fa697ec-6072-48ae-9061-eea15239cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up window title for display\n",
    "window_title = \"Camera Feed - Press 'q' to Quit\"\n",
    "\n",
    "# Configure camera source based on the 'use_csi' flag\n",
    "if use_csi:\n",
    "    # Use CSI camera with GStreamer pipeline\n",
    "    src = gstreamer_pipeline(sensor_id=sensor_id,\n",
    "                       display_width=capture_width,\n",
    "                       display_height=capture_height,\n",
    "                       flip_method=flip_method, \n",
    "                       capture_width=capture_width, \n",
    "                       capture_height=capture_height, \n",
    "                       framerate=framerate)\n",
    "    cv2_capture = cv2.VideoCapture(src)\n",
    "else:\n",
    "    # Use V4L2 camera\n",
    "    cv2_capture = cv2.VideoCapture(sensor_id, cv2.CAP_V4L2)\n",
    "    cv2_capture.set(cv2.CAP_PROP_FRAME_WIDTH, capture_width)\n",
    "    cv2_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, capture_height)\n",
    "    cv2_capture.set(cv2.CAP_PROP_FPS, framerate)\n",
    "\n",
    "# Create a FrameDropper object to handle video capture\n",
    "video_capture = FrameDropper(cv2_capture)\n",
    "\n",
    "# Initialize the ByteTracker for object tracking\n",
    "tracker = BYTETracker(track_thresh=0.25, track_buffer=30, match_thresh=0.8, frame_rate=30)\n",
    "\n",
    "try:\n",
    "    # Create a named window for displaying the video feed\n",
    "    window_handle = cv2.namedWindow(window_title, cv2.WINDOW_AUTOSIZE)\n",
    "    \n",
    "    # Main processing loop\n",
    "    while True:\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Capture a frame from the video feed\n",
    "        ret_val, frame = video_capture.read()\n",
    "        \n",
    "        if not ret_val:\n",
    "            print(\"Failed to retrieve frame\")\n",
    "            continue\n",
    "    \n",
    "        # Prepare the input image for inference\n",
    "        rgb_img, input_dims, offsets, min_img_scale, input_img = prepare_image_for_inference(frame, test_sz, max_stride)\n",
    "        \n",
    "        # Convert the input image to NumPy format for the model\n",
    "        input_tensor_np = np.array(input_img, dtype=np.float32).transpose((2, 0, 1))[None]/255\n",
    "                        \n",
    "        # Run inference using the ONNX session\n",
    "        outputs = session.run(None, {\"input\": input_tensor_np})[0]\n",
    "    \n",
    "        # Process the model output to get object proposals\n",
    "        proposals = process_outputs(outputs, input_tensor_np.shape[input_dim_slice], bbox_conf_thresh)\n",
    "        \n",
    "        # Apply non-max suppression to filter overlapping proposals\n",
    "        proposal_indices = nms_sorted_boxes(calc_iou(proposals[:, :-2]), iou_thresh)\n",
    "        proposals = proposals[proposal_indices]\n",
    "        \n",
    "        # Extract bounding boxes, labels, and probabilities from proposals\n",
    "        bbox_list = (proposals[:,:4]+[*offsets, 0, 0])*min_img_scale\n",
    "        label_list = [class_names[int(idx)] for idx in proposals[:,4]]\n",
    "        probs_list = proposals[:,5]\n",
    "\n",
    "        # Initialize track IDs for detected objects\n",
    "        track_ids = [-1]*len(bbox_list)\n",
    "\n",
    "        # Convert bounding boxes to top-left bottom-right (tlbr) format\n",
    "        tlbr_boxes = bbox_list.copy()\n",
    "        tlbr_boxes[:, 2:4] += tlbr_boxes[:, :2]\n",
    "\n",
    "        # Update tracker with detections\n",
    "        tracks = tracker.update(\n",
    "            output_results=np.concatenate([tlbr_boxes, probs_list[:, np.newaxis]], axis=1),\n",
    "            img_info=rgb_img.size,\n",
    "            img_size=rgb_img.size)\n",
    "\n",
    "        if len(tlbr_boxes) > 0 and len(tracks) > 0:\n",
    "            # Match detections with tracks\n",
    "            track_ids = match_detections_with_tracks(tlbr_boxes=tlbr_boxes, track_ids=track_ids, tracks=tracks)\n",
    "    \n",
    "            # Filter object detections based on tracking results\n",
    "            bbox_list, label_list, probs_list, track_ids = zip(*[(bbox, label, prob, track_id) \n",
    "                                                                for bbox, label, prob, track_id \n",
    "                                                                in zip(bbox_list, label_list, probs_list, track_ids) if track_id != -1])\n",
    "            \n",
    "            if len(bbox_list) > 0:\n",
    "                # Annotate the current frame with bounding boxes and tracking IDs\n",
    "                annotated_img = draw_bboxes_pil(\n",
    "                    image=rgb_img, \n",
    "                    boxes=bbox_list, \n",
    "                    labels=[f\"{track_id}-{label}\" for track_id, label in zip(track_ids, label_list)],\n",
    "                    probs=probs_list,\n",
    "                    colors=[int_colors[class_names.index(i)] for i in label_list],  \n",
    "                    font=font_file,\n",
    "                )\n",
    "                annotated_frame = cv2.cvtColor(np.array(annotated_img), cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            # If no detections, use the original frame\n",
    "            annotated_frame = frame\n",
    "    \n",
    "        # Calculate and display FPS\n",
    "        end_time = time.perf_counter()\n",
    "        processing_time = end_time - start_time\n",
    "        fps = 1 / processing_time\n",
    "    \n",
    "        fps_text = f\"FPS: {fps:.2f}\"\n",
    "        cv2.putText(annotated_frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(window_title, annotated_frame)\n",
    "        \n",
    "        # Check for 'q' key press to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Clean up resources\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3272b0-5065-4f56-9fd9-60c9872ce05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
