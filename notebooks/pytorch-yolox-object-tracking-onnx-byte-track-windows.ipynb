{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562e8e77",
   "metadata": {},
   "source": [
    "## Setting Up Your Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6569af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Install additional dependencies\n",
    "# !pip install pandas pillow opencv-python\n",
    "\n",
    "# # Install ONNX packages\n",
    "# !pip install onnx onnxruntime onnx-simplifier\n",
    "\n",
    "# # Install utility packages\n",
    "# !pip install cjm_psl_utils cjm_pil_utils cjm_byte_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f28533-cfe4-45d8-850e-a7c5063e508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U cjm_byte_track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f835e2b",
   "metadata": {},
   "source": [
    "## Importing the Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da56cda5-9fa1-4a9c-8f29-5f801c9c80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Standard Library dependencies\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "# Import ByteTrack package\n",
    "from cjm_byte_track.byte_tracker import BYTETracker\n",
    "from cjm_byte_track.matching import match_detections_with_tracks\n",
    "\n",
    "# Import utility functions\n",
    "from cjm_psl_utils.core import download_file\n",
    "from cjm_pil_utils.core import resize_img\n",
    "\n",
    "# Import OpenCV\n",
    "import cv2\n",
    "\n",
    "# Class for displaying videos in Jupyter notebooks\n",
    "from IPython.display import Video\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Import PIL for image manipulation\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Import ONNX dependencies\n",
    "import onnx # Import the onnx module\n",
    "import onnxruntime as ort # Import the ONNX Runtime\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2a086-291b-4d60-bf8a-025a60c49d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa8ce96",
   "metadata": {},
   "source": [
    "## Setting Up the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953f868",
   "metadata": {},
   "source": [
    "### Set the Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a2baf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_97051\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_97051_level0_row0\" class=\"row_heading level0 row0\" >Project Directory:</th>\n",
       "      <td id=\"T_97051_row0_col0\" class=\"data row0 col0\" >pytorch-yolox-object-detector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97051_level0_row1\" class=\"row_heading level0 row1\" >Checkpoint Directory:</th>\n",
       "      <td id=\"T_97051_row1_col0\" class=\"data row1 col0\" >pytorch-yolox-object-detector\\pretrained-coco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21d67bee350>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The name for the project\n",
    "project_name = f\"pytorch-yolox-object-detector\"\n",
    "\n",
    "# The path for the project folder\n",
    "project_dir = Path(f\"./{project_name}/\")\n",
    "\n",
    "# Create the project directory if it does not already exist\n",
    "project_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The path to the checkpoint folder\n",
    "# checkpoint_dir = Path(project_dir/f\"2023-08-17_16-14-43\")\n",
    "checkpoint_dir = Path(project_dir/f\"pretrained-coco\")\n",
    "\n",
    "pd.Series({\n",
    "    \"Project Directory:\": project_dir,\n",
    "    \"Checkpoint Directory:\": checkpoint_dir,\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a96378",
   "metadata": {},
   "source": [
    "### Download a Font File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37213df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./KFOlCnqEu92Fr1MmEU9vAw.ttf already exists and overwrite is set to False.\n"
     ]
    }
   ],
   "source": [
    "# Set the name of the font file\n",
    "font_file = 'KFOlCnqEu92Fr1MmEU9vAw.ttf'\n",
    "\n",
    "# Download the font file\n",
    "download_file(f\"https://fonts.gstatic.com/s/roboto/v30/{font_file}\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9abf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cd8dddc",
   "metadata": {},
   "source": [
    "## Loading the Checkpoint Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb7352",
   "metadata": {},
   "source": [
    "### Load the Colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3560146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The colormap path\n",
    "colormap_path = list(checkpoint_dir.glob('*colormap.json'))[0]\n",
    "\n",
    "# Load the JSON colormap data\n",
    "with open(colormap_path, 'r') as file:\n",
    "        colormap_json = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a dictionary        \n",
    "colormap_dict = {item['label']: item['color'] for item in colormap_json['items']}\n",
    "\n",
    "# Extract the class names from the colormap\n",
    "class_names = list(colormap_dict.keys())\n",
    "\n",
    "# Make a copy of the colormap in integer format\n",
    "int_colors = [tuple(int(c*255) for c in color) for color in colormap_dict.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ac296-253f-4ecf-baab-2ab62e095ea1",
   "metadata": {},
   "source": [
    "### Set the Preprocessing and Post-Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f83e7cf-dc3b-40f6-932b-5e42c70639ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stride = 32\n",
    "input_dim_slice = slice(2, 4, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f1cfd-6d21-4546-a860-f218e24661ab",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5960f50-446b-488c-810c-f1abf98431d4",
   "metadata": {},
   "source": [
    "### Define a Function to Prepare Images for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bf68c1-f023-46b7-96f0-e580854e4cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_for_inference(frame:np.ndarray, target_sz:int, max_stride:int):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepares an image for inference by performing a series of preprocessing steps.\n",
    "    \n",
    "    Steps:\n",
    "    1. Converts a BGR image to RGB.\n",
    "    2. Resizes the image to a target size without cropping, considering a given divisor.\n",
    "    3. Calculates input dimensions as multiples of the max stride.\n",
    "    4. Calculates offsets based on the resized image dimensions and input dimensions.\n",
    "    5. Computes the scale between the original and resized image.\n",
    "    6. Crops the resized image based on calculated input dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    - frame (numpy.ndarray): The input image in BGR format.\n",
    "    - target_sz (int): The target minimum size for resizing the image.\n",
    "    - max_stride (int): The maximum stride to be considered for calculating input dimensions.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: \n",
    "    - rgb_img (PIL.Image): The converted RGB image.\n",
    "    - input_dims (list of int): Dimensions of the image that are multiples of max_stride.\n",
    "    - offsets (numpy.ndarray): Offsets from the resized image dimensions to the input dimensions.\n",
    "    - min_img_scale (float): Scale factor between the original and resized image.\n",
    "    - input_img (PIL.Image): Cropped image based on the calculated input dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    # Resize image without cropping to multiple of the max stride\n",
    "    resized_img = resize_img(rgb_img, target_sz=target_sz, divisor=1)\n",
    "    \n",
    "    # Calculating the input dimensions that multiples of the max stride\n",
    "    input_dims = [dim - dim % max_stride for dim in resized_img.size]\n",
    "    # Calculate the offsets from the resized image dimensions to the input dimensions\n",
    "    offsets = (np.array(resized_img.size) - input_dims) / 2\n",
    "    # Calculate the scale between the source image and the resized image\n",
    "    min_img_scale = min(rgb_img.size) / min(resized_img.size)\n",
    "    \n",
    "    # Crop the resized image to the input dimensions\n",
    "    input_img = resized_img.crop(box=[*offsets, *resized_img.size - offsets])\n",
    "    \n",
    "    return rgb_img, input_dims, offsets, min_img_scale, input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ccc4ca-a907-4e0c-b03c-254815a58758",
   "metadata": {},
   "source": [
    "### Define Functions to Process YOLOX Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee2198-3ec1-4879-85ec-9a1cf9da7760",
   "metadata": {},
   "source": [
    "#### Define a function to generate the output grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b75909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_grids_np(height, width, strides=[8,16,32]):\n",
    "    \"\"\"\n",
    "    Generate a numpy array containing grid coordinates and strides for a given height and width.\n",
    "\n",
    "    Args:\n",
    "        height (int): The height of the image.\n",
    "        width (int): The width of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array containing grid coordinates and strides.\n",
    "    \"\"\"\n",
    "\n",
    "    all_coordinates = []\n",
    "\n",
    "    for stride in strides:\n",
    "        # Calculate the grid height and width\n",
    "        grid_height = height // stride\n",
    "        grid_width = width // stride\n",
    "\n",
    "        # Generate grid coordinates\n",
    "        g1, g0 = np.meshgrid(np.arange(grid_height), np.arange(grid_width), indexing='ij')\n",
    "\n",
    "        # Create an array of strides\n",
    "        s = np.full((grid_height, grid_width), stride)\n",
    "\n",
    "        # Stack the coordinates along with the stride\n",
    "        coordinates = np.stack((g0.flatten(), g1.flatten(), s.flatten()), axis=-1)\n",
    "\n",
    "        # Append to the list\n",
    "        all_coordinates.append(coordinates)\n",
    "\n",
    "    # Concatenate all arrays in the list along the first dimension\n",
    "    output_grids = np.concatenate(all_coordinates, axis=0)\n",
    "\n",
    "    return output_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46af61-23f2-41e8-866a-ea8de371cb0a",
   "metadata": {},
   "source": [
    "#### Define a function to calculate bounding boxes and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b471b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_boxes_and_probs(model_output:np.ndarray, output_grids:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the bounding boxes and their probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    model_output (numpy.ndarray): The output of the model.\n",
    "    output_grids (numpy.ndarray): The output grids.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The array containing the bounding box coordinates, class labels, and maximum probabilities.\n",
    "    \"\"\"\n",
    "    # Calculate the bounding box coordinates\n",
    "    box_centroids = (model_output[..., :2] + output_grids[..., :2]) * output_grids[..., 2:]\n",
    "    box_sizes = np.exp(model_output[..., 2:4]) * output_grids[..., 2:]\n",
    "\n",
    "    x0, y0 = [t.squeeze(axis=2) for t in np.split(box_centroids - box_sizes / 2, 2, axis=2)]\n",
    "    w, h = [t.squeeze(axis=2) for t in np.split(box_sizes, 2, axis=2)]\n",
    "\n",
    "    # Calculate the probabilities for each class\n",
    "    box_objectness = model_output[..., 4]\n",
    "    box_cls_scores = model_output[..., 5:]\n",
    "    box_probs = np.expand_dims(box_objectness, -1) * box_cls_scores\n",
    "\n",
    "    # Get the maximum probability and corresponding class for each proposal\n",
    "    max_probs = np.max(box_probs, axis=-1)\n",
    "    labels = np.argmax(box_probs, axis=-1)\n",
    "\n",
    "    return np.array([x0, y0, w, h, labels, max_probs]).transpose((1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0a016-8f3b-447e-86d8-fe343171e6e9",
   "metadata": {},
   "source": [
    "#### Define a function to extract object proposals from the raw model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "564870f9-06b1-4301-a0f5-d408ac209147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_outputs(outputs:np.ndarray, input_dims:tuple, bbox_conf_thresh:float):\n",
    "\n",
    "    \"\"\"\n",
    "    Process the model outputs to generate bounding box proposals filtered by confidence threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - outputs (numpy.ndarray): The raw output from the model, which will be processed to calculate boxes and probabilities.\n",
    "    - input_dims (tuple of int): Dimensions (height, width) of the input image to the model.\n",
    "    - bbox_conf_thresh (float): Threshold for the bounding box confidence/probability. Bounding boxes with a confidence\n",
    "                                score below this threshold will be discarded.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.array: An array of proposals where each proposal is an array containing bounding box coordinates\n",
    "                   and its associated probability, sorted in descending order by probability.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the model output\n",
    "    outputs = calculate_boxes_and_probs(outputs, generate_output_grids_np(*input_dims))\n",
    "    # Filter the proposals based on the confidence threshold\n",
    "    max_probs = outputs[:, :, -1]\n",
    "    mask = max_probs > bbox_conf_thresh\n",
    "    proposals = outputs[mask]\n",
    "    # Sort the proposals by probability in descending order\n",
    "    proposals = proposals[proposals[..., -1].argsort()][::-1]\n",
    "    return proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1207ea-09ed-42ed-9b3a-0495632666ad",
   "metadata": {},
   "source": [
    "#### Define a function to calculate the intersection-over-union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3cc1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(proposals:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) for all pairs of bounding boxes (x,y,w,h) in 'proposals'.\n",
    "\n",
    "    The IoU is a measure of overlap between two bounding boxes. It is calculated as the area of\n",
    "    intersection divided by the area of union of the two boxes.\n",
    "\n",
    "    Parameters:\n",
    "    proposals (2D np.array): A NumPy array of bounding boxes, where each box is an array [x, y, width, height].\n",
    "\n",
    "    Returns:\n",
    "    iou (2D np.array): The IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate coordinates for the intersection rectangles\n",
    "    x1 = np.maximum(proposals[:, 0], proposals[:, 0][:, None])\n",
    "    y1 = np.maximum(proposals[:, 1], proposals[:, 1][:, None])\n",
    "    x2 = np.minimum(proposals[:, 0] + proposals[:, 2], (proposals[:, 0] + proposals[:, 2])[:, None])\n",
    "    y2 = np.minimum(proposals[:, 1] + proposals[:, 3], (proposals[:, 1] + proposals[:, 3])[:, None])\n",
    "    \n",
    "    # Calculate intersection areas\n",
    "    intersections = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "\n",
    "    # Calculate union areas\n",
    "    areas = proposals[:, 2] * proposals[:, 3]\n",
    "    unions = areas[:, None] + areas - intersections\n",
    "\n",
    "    # Calculate IoUs\n",
    "    iou = intersections / unions\n",
    "\n",
    "    # Return the iou matrix\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00250d67-940c-4855-bbaa-32f3f0b63a7c",
   "metadata": {},
   "source": [
    "#### Define a function to filter bounding box proposals using Non-Maximum Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3ca40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_sorted_boxes(iou:np.ndarray, iou_thresh:float=0.45) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies non-maximum suppression (NMS) to sorted bounding boxes.\n",
    "\n",
    "    It suppresses boxes that have high overlap (as defined by the IoU threshold) with a box that \n",
    "    has a higher score.\n",
    "\n",
    "    Parameters:\n",
    "    iou (np.ndarray): An IoU matrix where each element i,j represents the IoU of boxes i and j.\n",
    "    iou_thresh (float): The IoU threshold for suppression. Boxes with IoU > iou_thresh are suppressed.\n",
    "\n",
    "    Returns:\n",
    "    keep (np.ndarray): The indices of the boxes to keep after applying NMS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask to keep track of boxes\n",
    "    mask = np.ones(iou.shape[0], dtype=bool)\n",
    "\n",
    "    # Apply non-max suppression\n",
    "    for i in range(iou.shape[0]):\n",
    "        if mask[i]:\n",
    "            # Suppress boxes with higher index and IoU > threshold\n",
    "            mask[(iou[i] > iou_thresh) & (np.arange(iou.shape[0]) > i)] = False\n",
    "\n",
    "    # Return the indices of the boxes to keep\n",
    "    return np.arange(iou.shape[0])[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2cb686-21b6-4edf-9b5f-ff2ee78674b6",
   "metadata": {},
   "source": [
    "### Define a Function to Annotate Images with Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d91cea1-7057-442f-949b-b1b2af44b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes_pil(image, boxes, labels, colors, font, width=2, font_size=18, probs=None):\n",
    "    \"\"\"\n",
    "    Annotates an image with bounding boxes, labels, and optional probability scores.\n",
    "\n",
    "    Parameters:\n",
    "    - image (PIL.Image): The input image on which annotations will be drawn.\n",
    "    - boxes (list of tuples): A list of bounding box coordinates where each tuple is (x, y, w, h).\n",
    "    - labels (list of str): A list of labels corresponding to each bounding box.\n",
    "    - colors (list of str): A list of colors for each bounding box and its corresponding label.\n",
    "    - font (str): Path to the font file to be used for displaying the labels.\n",
    "    - width (int, optional): Width of the bounding box lines. Defaults to 2.\n",
    "    - font_size (int, optional): Size of the font for the labels. Defaults to 18.\n",
    "    - probs (list of float, optional): A list of probability scores corresponding to each label. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    - annotated_image (PIL.Image): The image annotated with bounding boxes, labels, and optional probability scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a reference diagonal\n",
    "    REFERENCE_DIAGONAL = 1000\n",
    "    \n",
    "    # Scale the font size using the hypotenuse of the image\n",
    "    font_size = int(font_size * (np.hypot(*image.size) / REFERENCE_DIAGONAL))\n",
    "    \n",
    "    # Add probability scores to labels if provided\n",
    "    if probs is not None:\n",
    "        labels = [f\"{label}: {prob*100:.2f}%\" for label, prob in zip(labels, probs)]\n",
    "\n",
    "    # Create an ImageDraw object for drawing on the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load the font file (outside the loop)\n",
    "    fnt = ImageFont.truetype(font, font_size)\n",
    "    \n",
    "    # Compute the mean color value for each color\n",
    "    mean_colors = [np.mean(np.array(color)) for color in colors]\n",
    "\n",
    "    # Loop through the bounding boxes, labels, and colors\n",
    "    for box, label, color, mean_color in zip(boxes, labels, colors, mean_colors):\n",
    "        # Get the bounding box coordinates\n",
    "        x, y, w, h = box\n",
    "\n",
    "        # Draw the bounding box on the image\n",
    "        draw.rectangle([x, y, x+w, y+h], outline=color, width=width)\n",
    "        \n",
    "        # Get the size of the label text box\n",
    "        label_w, label_h = draw.textbbox(xy=(0,0), text=label, font=fnt)[2:]\n",
    "        \n",
    "        # Draw the label rectangle on the image\n",
    "        draw.rectangle([x, y-label_h, x+label_w, y], outline=color, fill=color)\n",
    "\n",
    "        # Draw the label text on the image\n",
    "        font_color = 'black' if mean_color > 127.5 else 'white'\n",
    "        draw.multiline_text((x, y-label_h), label, font=fnt, fill=font_color)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a83e62-ab33-48cb-91ed-71c4e814276b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fffabb13",
   "metadata": {},
   "source": [
    "## Tracking Objects in Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837f4dd",
   "metadata": {},
   "source": [
    "### Create an Inference Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8719b1c2-17de-4842-b9e1-3fee5ba3f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a filename for the ONNX model\n",
    "onnx_file_path = list(checkpoint_dir.glob('*.onnx'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "025f7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and create an InferenceSession\n",
    "providers = [\n",
    "    'CPUExecutionProvider',\n",
    "    # \"CUDAExecutionProvider\",\n",
    "]\n",
    "sess_options = ort.SessionOptions()\n",
    "session = ort.InferenceSession(onnx_file_path, sess_options=sess_options, providers=providers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09734ca-4a78-4e61-85b2-8394ff105140",
   "metadata": {},
   "source": [
    "### Select a Test Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "160329b9-8b4c-4192-b339-d64da3afe865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ./videos/cars_on_highway.mp4 already exists and overwrite is set to False.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./videos/cars_on_highway.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the directory where videos are or will be stored.\n",
    "video_dir = \"./videos/\"\n",
    "\n",
    "# Name of the test video to be used.\n",
    "# test_video_name = \"pexels-rodnae-productions-10373924.mp4\"\n",
    "test_video_name = \"cars_on_highway.mp4\"\n",
    "\n",
    "# Construct the full path for the video using the directory and video name.\n",
    "video_path = f\"{video_dir}{test_video_name}\"\n",
    "\n",
    "# Define the URL for the test video stored on Huggingface's server.\n",
    "test_video_url = f\"https://huggingface.co/datasets/cj-mills/pexels-object-tracking-test-videos/resolve/main/{test_video_name}\"\n",
    "\n",
    "# Download the video file from the specified URL to the local video directory.\n",
    "download_file(test_video_url, video_dir, False)\n",
    "\n",
    "# Display the video using the Video function (assuming an appropriate library/module is imported).\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8186db3-de33-4877-b937-a3f80e362ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f80d619c-2b6d-466d-8b2d-32201aa927d8",
   "metadata": {},
   "source": [
    "### Initialize a `VideoCapture` Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "998946c4-d5cb-4e00-a0cb-ba647fa36037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6fbc6\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6fbc6_level0_row0\" class=\"row_heading level0 row0\" >Frame Width:</th>\n",
       "      <td id=\"T_6fbc6_row0_col0\" class=\"data row0 col0\" >1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6fbc6_level0_row1\" class=\"row_heading level0 row1\" >Frame Height:</th>\n",
       "      <td id=\"T_6fbc6_row1_col0\" class=\"data row1 col0\" >720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6fbc6_level0_row2\" class=\"row_heading level0 row2\" >Frame FPS:</th>\n",
       "      <td id=\"T_6fbc6_row2_col0\" class=\"data row2 col0\" >50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6fbc6_level0_row3\" class=\"row_heading level0 row3\" >Frames:</th>\n",
       "      <td id=\"T_6fbc6_row3_col0\" class=\"data row3 col0\" >3000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21d67c34550>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the video file located at 'video_path' using OpenCV\n",
    "video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Retrieve the frame width of the video\n",
    "frame_width = int(video_capture.get(3))\n",
    "# Retrieve the frame height of the video\n",
    "frame_height = int(video_capture.get(4))\n",
    "# Retrieve the frames per second (FPS) of the video\n",
    "frame_fps = int(video_capture.get(5))\n",
    "# Retrieve the total number of frames in the video\n",
    "frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Create a pandas Series containing video metadata and convert it to a DataFrame\n",
    "pd.Series({\n",
    "    \"Frame Width:\": frame_width,\n",
    "    \"Frame Height:\": frame_height,\n",
    "    \"Frame FPS:\": frame_fps,\n",
    "    \"Frames:\": frames\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f95783-453d-4d47-9f2f-485feff3e0cc",
   "metadata": {},
   "source": [
    "### Initialize a `VideoWriter` Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edda2696-8083-472b-af49-2acd48842533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the output video path \n",
    "video_out_path = f\"{(video_dir)}{Path(video_path).stem}-byte-track.mp4\"\n",
    "\n",
    "# Initialize a VideoWriter object for video writing.\n",
    "# 1. video_out_path: Specifies the name of the output video file.\n",
    "# 2. cv2.VideoWriter_fourcc(*'mp4v'): Specifies the codec for the output video. 'mp4v' is used for .mp4 format.\n",
    "# 3. frame_fps: Specifies the frames per second for the output video.\n",
    "# 4. (frame_width, frame_height): Specifies the width and height of the frames in the output video.\n",
    "video_writer = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'mp4v'), frame_fps, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6913d8-1a72-4af8-a82e-042aea9d3b9a",
   "metadata": {},
   "source": [
    "### Define Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7deefea6-baeb-4414-bfab-16cce20cec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sz = 288\n",
    "# test_sz = 384\n",
    "bbox_conf_thresh = 0.1\n",
    "iou_thresh = 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e23fe0-2760-49af-89c6-341c1636c22a",
   "metadata": {},
   "source": [
    "### Detect, Track, and Annotate Objects in Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df0385fc-2632-41d3-8933-bf834f37c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15059748-9183-4062-bbbb-c1c8c5724fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "# Initialize a ByteTracker object\n",
    "tracker = BYTETracker(track_thresh=0.25, track_buffer=30, match_thresh=0.8, frame_rate=frame_fps)\n",
    "\n",
    "frame_counter = 0\n",
    "\n",
    "with tqdm(total=frames, desc=\"Processing frames\") as pbar:\n",
    "    # Iterate through each frame in the video\n",
    "    while video_capture.isOpened():\n",
    "        ret, frame = video_capture.read()\n",
    "        if ret:\n",
    "            frame_counter += 1\n",
    "            \n",
    "            # Prepare an input image for inference\n",
    "            rgb_img, input_dims, offsets, min_img_scale, input_img = prepare_image_for_inference(frame, test_sz, max_stride)\n",
    "                        \n",
    "            # Convert the existing input image to NumPy format\n",
    "            input_tensor_np = np.array(input_img, dtype=np.float32).transpose((2, 0, 1))[None]/255\n",
    "\n",
    "            # Start performance counter\n",
    "            start_time = time.perf_counter()\n",
    "                        \n",
    "            # Run inference\n",
    "            outputs = session.run(None, {\"input\": input_tensor_np})[0]\n",
    "\n",
    "            # Process the model output\n",
    "            proposals = process_outputs(outputs, input_tensor_np.shape[input_dim_slice], bbox_conf_thresh)\n",
    "            \n",
    "            # Apply non-max suppression to the proposals with the specified threshold\n",
    "            proposal_indices = nms_sorted_boxes(calc_iou(proposals[:, :-2]), iou_thresh)\n",
    "            proposals = proposals[proposal_indices]\n",
    "            \n",
    "            bbox_list = (proposals[:,:4]+[*offsets, 0, 0])*min_img_scale\n",
    "            label_list = [class_names[int(idx)] for idx in proposals[:,4]]\n",
    "            probs_list = proposals[:,5]\n",
    "\n",
    "            # Update tracker with detections.\n",
    "            track_ids = [-1]*len(bbox_list)\n",
    "\n",
    "            # Convert to tlbr format\n",
    "            tlbr_boxes = bbox_list.copy()\n",
    "            tlbr_boxes[:, 2:4] += tlbr_boxes[:, :2]\n",
    "\n",
    "            # print(np.concatenate([tlbr_boxes, probs_list[:, np.newaxis]], axis=1))\n",
    "\n",
    "            # Update tracker with detections\n",
    "            tracks = tracker.update(\n",
    "                output_results=np.concatenate([tlbr_boxes, probs_list[:, np.newaxis]], axis=1),\n",
    "                img_info=rgb_img.size,\n",
    "                img_size=rgb_img.size)\n",
    "            track_ids = match_detections_with_tracks(tlbr_boxes=tlbr_boxes, track_ids=track_ids, tracks=tracks)\n",
    "            print(f\"\\nFRAME-{frame_counter}: {track_ids}\")\n",
    "            # if (frame_counter >= 300): \n",
    "            #     break\n",
    "\n",
    "            # End performance counter\n",
    "            end_time = time.perf_counter()\n",
    "            # Calculate the combined FPS for object detection and tracking\n",
    "            fps = 1 / (end_time - start_time)\n",
    "            # Display the frame rate in the progress bar\n",
    "            pbar.set_postfix(fps=fps)\n",
    "\n",
    "            # Filter object detections based on tracking results\n",
    "            bbox_list, label_list, probs_list, track_ids = zip(*[(bbox, label, prob, track_id) \n",
    "                                                                 for bbox, label, prob, track_id \n",
    "                                                                 in zip(bbox_list, label_list, probs_list, track_ids) if track_id != -1])\n",
    "\n",
    "            # Annotate the current frame with bounding boxes and tracking IDs\n",
    "            annotated_img = draw_bboxes_pil(\n",
    "                image=rgb_img, \n",
    "                boxes=bbox_list, \n",
    "                labels=[f\"{track_id}-{label}\" for track_id, label in zip(track_ids, label_list)],\n",
    "                probs=probs_list,\n",
    "                colors=[int_colors[class_names.index(i)] for i in label_list],  \n",
    "                font=font_file,\n",
    "            )\n",
    "            annotated_frame = cv2.cvtColor(np.array(annotated_img), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            video_writer.write(annotated_frame)\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            break\n",
    "video_capture.release()\n",
    "video_writer.release()\n",
    "\n",
    "# with open('output.md', 'w') as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4d2f655-2912-408c-a666-a1723add11f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "with open('output.md', 'w') as f:\n",
    "    f.write(cap.stdout)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2078cfc6-a32d-4f1f-b4a5-9ee321677528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cjm_byte_track.matching import ious, linear_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb567673-f097-482f-b28a-51d2e7ad8076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_distance_test(stracks, detections):\n",
    "    atlbrs = [track+np.array([0,0, *track[:2]]) for track in stracks]\n",
    "    btlbrs = [track+np.array([0,0, *track[:2]]) for track in detections]\n",
    "    return 1 - ious(atlbrs, btlbrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c2cbbc3-bfa1-40f1-afcc-0bf4affb9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "stracks = [np.array([370.46357155, 877.45772362, 145.92466354, 119.21984673])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9727a9f-90a1-46e7-95e9-777f439fa30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = [np.array([368.32773685, 879.60356593, 143.66029739, 114.70830917]), np.array([240.97084284, 833.63639474, 148.40068817, 133.82930756])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3579153-bef9-49ba-8d03-dabf4c102676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07981369, 0.95213506]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_distance_test(stracks, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beadc0fb-f36f-425d-a282-44402d16b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_thresh=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ad713a4-7b1e-496e-bf4b-16a590918755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_to_matches\n",
      "indices:\n",
      "[[0 0]\n",
      " [1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [1, 1]], dtype=int64),\n",
       " (),\n",
       " ())"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = np.array([[0.09774083, 1],\n",
    "         [1, 0.18316322]])\n",
    "linear_assignment(dists, match_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56b824a8-df3b-48fe-b05e-9def7e82ac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_to_matches\n",
      "indices:\n",
      "[[0 0]\n",
      " [1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1, 1]], dtype=int64), (0,), (0,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = np.array([[1, 1],\n",
    "         [1, 0.173594]])\n",
    "linear_assignment(dists, match_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2898a773-7cce-481d-ab20-8b6d85d5a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_to_matches\n",
      "indices:\n",
      "[[0 0]\n",
      " [1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [1, 1]], dtype=int64),\n",
       " (),\n",
       " ())"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = np.array([[0.0631399, 0.986465],\n",
    "                  [0.96626, 0.111179]])\n",
    "linear_assignment(dists, match_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac3d06ed-dfdf-4ef6-b757-bb6408013099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestClass:\n",
    "    def __init__(self):\n",
    "        self.item = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8a4b501-5f5e-4e0e-96d6-5524377dcc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [TestClass() for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07000f3b-82c9-43c4-a417-3ce73036f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_changer(test_list):\n",
    "    return test_list[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "259f673f-0552-429f-ae2c-226b81785e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = list_changer(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "135cfc40-86c6-49ba-802c-055ad065dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in new_list:\n",
    "    cls.item += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aa9a6fe-cc06-43c6-adc5-0d46e1092cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cls.item for cls in new_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5dd4b62-9f99-4a9b-b43f-c86a3a4cd336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cls.item for cls in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aab556-bead-4f17-a525-79b33db903b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c11fd4b5-d298-46c9-a962-ac793d1492ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_thresh=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e524c90-3d0e-4c6f-a95c-a22d78b45431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_to_matches\n",
      "indices:\n",
      "[[0 0]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [3 5]\n",
      " [4 4]\n",
      " [5 8]\n",
      " [6 6]\n",
      " [7 3]\n",
      " [8 7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [3, 5],\n",
       "        [4, 4],\n",
       "        [6, 6],\n",
       "        [7, 3],\n",
       "        [8, 7]], dtype=int64),\n",
       " (5,),\n",
       " (8,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the values as a list of lists, with each inner list representing a row\n",
    "values = [\n",
    "    [0.0673455, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0.0600082, 1, 0.967432, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 0.025685, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 0.0627941, 1, 1, 1],\n",
    "    [1, 1, 1, 0.74512, 0.0319988, 1, 0.836532, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 0.976879, 0.824716, 1, 0.0832212, 1, 0.942253],\n",
    "    [1, 0.972737, 1, 0.0347837, 0.733537, 1, 0.981221, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 0.0162111, 1]\n",
    "]\n",
    "\n",
    "# Convert the list of lists to a numpy array\n",
    "dists = np.array(values)\n",
    "\n",
    "linear_assignment(dists, match_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c138c4e-7dc9-4729-9d10-b3ac87adadee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_to_matches\n",
      "indices:\n",
      "[[0 0]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [3 5]\n",
      " [4 3]\n",
      " [5 6]\n",
      " [6 9]\n",
      " [7 4]\n",
      " [8 8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [3, 5],\n",
       "        [4, 3],\n",
       "        [5, 6],\n",
       "        [6, 9],\n",
       "        [7, 4],\n",
       "        [8, 8]], dtype=int64),\n",
       " (),\n",
       " (7,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = np.array([\n",
    "  [0.06871448, 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "  [1., 0.06176221, 1., 1., 0.96813791, 1., 1., 1., 1., 1.],\n",
    "  [1., 1., 0.02992284, 1., 1., 1., 1., 1., 1., 1.],\n",
    "  [1., 1., 1., 1., 1., 0.055438, 1., 1., 1., 1.],\n",
    "  [1., 1., 1., 0.03692986, 0.74497972, 1., 0.80186697, 1., 1., 1.],\n",
    "  [1., 1., 1., 0.82536749, 0.97627397, 1., 0.0555338, 0.93578798, 1., 1.],\n",
    "  [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.03617145],\n",
    "  [1., 0.97301484, 1., 0.72932389, 0.02848313, 1., 0.96989549, 1., 1., 1.],\n",
    "  [1., 1., 1., 1., 1., 1., 1., 1., 0.01484608, 1.]\n",
    "])\n",
    "\n",
    "linear_assignment(dists, match_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fba3209-043a-424d-9b34-7b5e020f0543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_to_matches\n",
      "indices:\n",
      "[[2 3]\n",
      " [4 2]\n",
      " [5 1]\n",
      " [6 0]\n",
      " [7 6]\n",
      " [8 5]\n",
      " [9 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2, 3],\n",
       "        [4, 2],\n",
       "        [5, 1],\n",
       "        [6, 0],\n",
       "        [7, 6],\n",
       "        [8, 5],\n",
       "        [9, 4]], dtype=int64),\n",
       " (0, 1, 3, 10, 11),\n",
       " ())"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = np.array([\n",
    "    [1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 0.941002, 1, 1, 1, 0.575342],\n",
    "    [1, 1, 1, 0.0317649, 0.950865, 1, 1],\n",
    "    [1, 1, 0.908129, 1, 1, 1, 0.677881],\n",
    "    [1, 1, 0.031623, 1, 1, 1, 0.845811],\n",
    "    [1, 0.0319127, 1, 1, 1, 1, 1],\n",
    "    [0.034943, 1, 0.999031, 1, 1, 1, 1],\n",
    "    [1, 1, 0.856714, 1, 1, 1, 0.0713416],\n",
    "    [1, 1, 1, 1, 1, 0.00747945, 1],\n",
    "    [1, 1, 1, 0.955944, 0.0108065, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1],\n",
    "    [0.96013, 1, 1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "linear_assignment(dists, match_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f25b676a-8515-4d8c-8588-e09712c61494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_to_matches\n",
      "indices:\n",
      "[[0 0]\n",
      " [1 1]\n",
      " [2 3]\n",
      " [3 2]\n",
      " [4 5]\n",
      " [5 8]\n",
      " [6 6]\n",
      " [7 4]\n",
      " [8 7]\n",
      " [9 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 3],\n",
       "        [3, 2],\n",
       "        [4, 5],\n",
       "        [5, 8],\n",
       "        [6, 6],\n",
       "        [7, 4],\n",
       "        [8, 7],\n",
       "        [9, 9]], dtype=int64),\n",
       " (),\n",
       " ())"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = np.array([\n",
    "    [0.0583072, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0.0432461, 1, 1, 0.968647, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 0.0128587, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 0.0354033, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 0.74248, 0.0344458, 0.811372, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 0.0406445, 1],\n",
    "    [1, 1, 1, 1, 0.976859, 0.826129, 0.0430449, 1, 1, 0.954299],\n",
    "    [1, 0.971974, 1, 1, 0.0414572, 0.732245, 0.974159, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 0.0109232, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 0.963612, 1, 1, 0.146502]\n",
    "])\n",
    "\n",
    "linear_assignment(dists, match_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcc5bc-5581-4747-bf74-ad0bbc70bf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18b5ff58-0a85-463c-b2e0-4c28748ef467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cjm_byte_track.matching import box_iou_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4bec26e-7c7f-43b6-98f7-96cac110b96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05830749, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 0.04324753, 1.        , 1.        , 0.9686473 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 0.01287833, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 0.03540396, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 0.74247683,\n",
       "        0.03444314, 0.8113688 , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.04064818, 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 0.97685778,\n",
       "        0.82612748, 0.04305579, 1.        , 1.        , 0.9543001 ],\n",
       "       [1.        , 0.97197521, 1.        , 1.        , 0.04145012,\n",
       "        0.73224479, 0.97415874, 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.01091631, 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.9636119 , 1.        , 1.        , 0.14649402]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stracks_tlbr = [\n",
    "    [802.431, 301.6, 995.954, 451.851],\n",
    "    [521.554, 369.363, 711.1, 529.451],\n",
    "    [191.288, 193.669, 267.159, 244],\n",
    "    [565.679, 566.005, 815.292, 719.111],\n",
    "    [389.009, 224.114, 508.233, 353.844],\n",
    "    [597.513, 193.034, 737.378, 309.714],\n",
    "    [326.345, 189.51, 447.15, 298.353],\n",
    "    [420.268, 270.779, 585.164, 393.233],\n",
    "    [88.4078, 136.008, 178.812, 253.103],\n",
    "    [269.081, 208.368, 343.869, 241.893]\n",
    "]\n",
    "\n",
    "# Create a NumPy array from the data\n",
    "stracks_tlbr_np = np.ascontiguousarray(np.array(stracks_tlbr))\n",
    "\n",
    "\n",
    "detections_tlbr = [\n",
    "    [805.686, 304.402, 996.72, 455.002],\n",
    "    [524.17, 370.633, 715.037, 529.768],\n",
    "    [567.432, 569.344, 815.9, 719.817],\n",
    "    [191.558, 193.821, 267.597, 244.032],\n",
    "    [421.328, 268.753, 587.79, 392.764],\n",
    "    [388.84, 224.5, 511.07, 354.787],\n",
    "    [327.209, 190.112, 449.947, 299.291],\n",
    "    [88.2511, 136.163, 178.74, 253.941],\n",
    "    [599.344, 192.703, 739.006, 308.079],\n",
    "    [268.088, 204.027, 345.001, 241.598]\n",
    "]\n",
    "\n",
    "# Create a NumPy array from the detections data\n",
    "detections_tlbr_np = np.ascontiguousarray(np.array(detections_tlbr))\n",
    "\n",
    "1-box_iou_batch(stracks_tlbr_np, detections_tlbr_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd047b-b048-4e47-acbe-d5db3d3e9a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7a34d33-9453-42c9-987b-b7c81d349828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05394472, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 0.03939261, 1.        , 1.        , 0.96888572,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 0.02049892, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 0.0442709 , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 0.74190436,\n",
       "        0.02437249, 0.80984901, 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 0.97639729,\n",
       "        0.82357392, 0.03459486, 1.        , 1.        , 0.94949333],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.03905921, 1.        , 1.        ],\n",
       "       [1.        , 0.97079682, 1.        , 1.        , 0.0327856 ,\n",
       "        0.73137128, 0.97234661, 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.00810766, 1.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stracks_tlbr = [[801.68070078, 301.69291735, 995.83788157, 451.876719],\n",
    "                 [521.22518063, 369.33599353, 711.42142773, 529.38341022],\n",
    "                 [191.16331458, 193.47710609, 266.96166396, 243.86285782],\n",
    "                 [565.75703144, 565.5274415, 815.41626453, 719.13038492],\n",
    "                 [388.49719703, 223.79027367, 509.01418388, 353.95997047],\n",
    "                 [326.05074883, 189.85471725, 447.58558273, 298.98592949],\n",
    "                 [597.48936176, 192.86307931, 737.04803944, 310.09313226],\n",
    "                 [419.81409431, 269.17650223, 585.61533332, 393.53150368],\n",
    "                 [87.96551704, 135.90763569, 178.39156151, 254.06175137]]\n",
    "\n",
    "# Create a NumPy array from the data\n",
    "stracks_tlbr_np = np.ascontiguousarray(np.array(stracks_tlbr))\n",
    "\n",
    "\n",
    "detections_tlbr = [[804.66910124, 304.01881218, 996.75326109, 454.91154671],\n",
    "        [523.68838668, 370.06069183, 715.21527648, 529.86287117],\n",
    "        [567.03765035, 570.35922289, 816.41314626, 719.79817629],\n",
    "        [191.39453053, 193.88383031, 267.48803258, 243.99884343],\n",
    "        [421.60306096, 269.01772976, 587.72294164, 392.45414257],\n",
    "        [388.93204629, 225.01641512, 510.41085184, 353.98362398],\n",
    "        [327.04007626, 190.07407188, 449.82983112, 299.73217964],\n",
    "        [599.40533876, 192.85640478, 738.88836145, 308.57549429],\n",
    "        [ 87.77255535, 135.95619678, 178.5406065 , 254.53011036],\n",
    "        [267.82703042, 201.37670994, 345.64222932, 241.60795689]]\n",
    "\n",
    "# Create a NumPy array from the detections data\n",
    "detections_tlbr_np = np.ascontiguousarray(np.array(detections_tlbr))\n",
    "\n",
    "1-box_iou_batch(stracks_tlbr_np, detections_tlbr_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ba718-1a7e-4d7b-9471-e29750e19068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
